{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/marl_routing/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from gym import Env, spaces\n",
    "import unittest\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import collections\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "from gym.envs.registration import EnvSpec, register\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.dqn.dqn_policy_graph import *\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import *\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "from ray.rllib.models.preprocessors import DictFlatteningPreprocessor, Preprocessor\n",
    "\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/Users/mtgibson/learning_wardrop/graph_results.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BraessEnv(MultiAgentEnv):\n",
    "    \"\"\"Traffic Environment that uses the Braess's network. \n",
    "       See https://github.com/openai/gym/blob/master/gym/core.py for more details.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, _):\n",
    "        # Make the Braess's network\n",
    "        self.network = BraessNetwork()\n",
    "        self.routes = self.network.routes\n",
    "        \n",
    "        # Observation space contains each route and travel times\n",
    "        self.num_routes = len(self.routes)\n",
    "        self.observation_space = spaces.Box(low=0,high=float('+inf'),shape=(3,),dtype=np.float32)\n",
    "        \n",
    "        # Action space contains each route and flow distribution of population (decimal)\n",
    "        self.action_space = action_spaces = spaces.Box(low=0,high=1,shape=(3,),dtype=np.float32)\n",
    "        \n",
    "        self.reward_range = (-float('inf'), 0)\n",
    "        \n",
    "        #Storage bins for data\n",
    "        self.__path_flows = []\n",
    "        self.__travel_times = []\n",
    "        self.__avg_times = []\n",
    "        return\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        \"\"\"Run one timestep of the environment's dynamics. \n",
    "        \n",
    "        Env Step Procedure:\n",
    "            (1) takes in routing distribution - comes from the action, \n",
    "            (2) calculate travel times for each path given flow on each path, \n",
    "            (3) return the reward which is the negative of the travel time.\n",
    "            \n",
    "        Note: We will give a reward to each path and will return an array/dictionary \n",
    "              as the reward for the population.\n",
    "            \n",
    "        Args:\n",
    "            action (dictionary): A dictionary where the key is a population and values are another dictionary where\n",
    "                                 the key is a path and the value is a flow distribution.\n",
    "                                 We assume there is only 1 agent and thus one o-d pair\n",
    "        \n",
    "        Returns:\n",
    "            next_observation (array): the travel times determined for each path\n",
    "            reward (float): -1*travel_time_of_agent\n",
    "            done (boolean): _\n",
    "            info (dict): other information needed - don't really need now though\n",
    "        \"\"\"\n",
    "        obs_dict, rew_dict, done, info_dict = {}, {}, {}, {}\n",
    "        \n",
    "        \n",
    "        for agent, action in action_dict.items():\n",
    "            # Create action dictionary\n",
    "            action_dict = {}\n",
    "            for i in range(len(action)):\n",
    "                action_dict[self.routes[i]] = action[i]\n",
    "            \n",
    "            # Calculate the travel times and store flow distributions and travel times (Edit for Multi-agent)\n",
    "            travel_times_dict = self.network.calculate_ttime(action_dict)\n",
    "            self.__travel_times.append(travel_times_dict)\n",
    "            \n",
    "            #Transform dictionary into list\n",
    "            travel_times = []\n",
    "            for route in self.routes:\n",
    "                travel_times.append(travel_times_dict[route])\n",
    "            \n",
    "            # Calculate the reward for the population - mean (negative) travel time (Edit for MA w/ different routes)\n",
    "            reward = np.dot(np.array(action), \n",
    "                            -1*np.array(travel_times))\n",
    "            \n",
    "            obs_dict[agent] = np.array(travel_times) \n",
    "            rew_dict[agent] = reward\n",
    "            done[agent] = True\n",
    "            info_dict[agent] = {}\n",
    "            \n",
    "            # Add data to the storage bins\n",
    "            self.add_data(action, travel_times, reward)\n",
    "                            \n",
    "        done[\"__all__\"] = True\n",
    "        return obs_dict, rew_dict, done, info_dict\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the state of the environment and returns an initial observation.\n",
    "        \n",
    "        For the initial observation: Make an array of 3 elements corresponding to 3 paths in\n",
    "        the Braess network. It should have the format ---\n",
    "        \n",
    "        state = [<traveltime_ABD>, <traveltime_ACD>, <traveltime_ABCD>] = [2, 2, 0.25]\n",
    "        \"\"\"\n",
    "        # Calculate initial travel times with 0 flow on the network\n",
    "        flows = {route: 0 for route in self.routes}\n",
    "        initial_dict = self.network.calculate_ttime(flows)\n",
    "        \n",
    "        # Turn initial observation to an array\n",
    "        t_0 = []\n",
    "        for route in self.routes:\n",
    "            t_0.append(initial_dict[route])\n",
    "        \n",
    "        # State will be a numpy array\n",
    "        self.state = {'population_1': np.array(t_0)}\n",
    "        \n",
    "        # Reset the data bins and insert first item\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "        self.file = open(filename, \"a+\")\n",
    "        self.file.write(str(t_0) + \"\\n\")\n",
    "        self.file.close()\n",
    "        \n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    def add_data(self, path_flow, travel_time, reward):\n",
    "        self.file = open(filename, \"a\")\n",
    "        self.file.write(str(path_flow) + ';' + str(travel_time) + ';' + str(reward) + '\\n')\n",
    "        self.file.close()\n",
    "        return\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BraessNetwork(object):\n",
    "    \"\"\"Stores the cost for all links. Handles calculating the cost of a path given action\n",
    "       of every car.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.__links = {\n",
    "            \"AB\": lambda f: 1 + (f/100),\n",
    "            \"AC\": lambda _: 2,\n",
    "            \"BD\": lambda _: 2,\n",
    "            \"CD\": lambda f: 1 + (f/100),\n",
    "            \"BC\": lambda _: 0.25\n",
    "        } # Dictionary of links and their congestion functions\n",
    "        self.__paths = {\n",
    "            \"ABD\": (\"AB\", \"BD\"),\n",
    "            \"ACD\": (\"AC\", \"CD\"),\n",
    "            \"ABCD\": (\"AB\", \"BC\", \"CD\")\n",
    "        } # Dictionaries of paths to links\n",
    "        self.total_flow = 100  # 100 cars in total on this network\n",
    "        return \n",
    "    \n",
    "    @property\n",
    "    def routes(self):\n",
    "        \"\"\"Gives a list of all possible paths in the network to the environment. \n",
    "           The environment could then assign an action number to each path. \n",
    "        \"\"\"\n",
    "        return (\"ABD\", \"ACD\", \"ABCD\")\n",
    "    \n",
    "    def calculate_ttime(self, flows):\n",
    "        \"\"\"Given a dictionary of paths and flows, this function returns a dictionary of \n",
    "           paths and travel time (secs), a.k.a ttime.\n",
    "           \n",
    "           Arg:\n",
    "               flows (dictionary): A dictionare where the key correspond to a path in the network of one o-d pair\n",
    "                                   and the value corresponds to the flow on that path. Flow will be a float\n",
    "                                   between 0 and 1 represent the percent of flow. \n",
    "           \n",
    "           Returns: \n",
    "               travel_times (list): A list of travel times, order matters \n",
    "                                    --> according to the order in my list of paths.\n",
    "        \"\"\"\n",
    "        congestion = {}\n",
    "        for path in flows:\n",
    "            links = self.__paths[path]\n",
    "            for link in links:\n",
    "                if link not in congestion:\n",
    "                    congestion[link] = 0\n",
    "                congestion[link] += flows[path] * self.total_flow\n",
    "        \n",
    "        t_time = {}\n",
    "        for path in flows:\n",
    "            total_time = 0\n",
    "            # Calculate travel time of path by adding the congestion time of every \n",
    "            # link in that path\n",
    "            links = self.__paths[path]\n",
    "            for link in links:\n",
    "                t_time_func = self.__links[link]\n",
    "                total_time += t_time_func(congestion[link])\n",
    "            t_time[path] = total_time\n",
    "        \n",
    "        \n",
    "        return t_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the environments\n",
    "# All environments in this project\n",
    "envs = {\"Braess\": BraessEnv\n",
    "        \"2PopEnv\": #Insert the class name of the Yiling made}\n",
    "        \n",
    "def env_constructor(params, version=0, render=None):\n",
    "    \"\"\"Return a constructor from make_create_env.\"\"\"\n",
    "    create_env, env_name = make_create_env(params, version, render)\n",
    "    return create_env\n",
    "        \n",
    "def make_create_env(params, version=0, render=None):\n",
    "    \"\"\"From Flow-repo ----\n",
    "    Create a parametrized flow environment compatible with OpenAI gym.\n",
    "    This environment creation method allows for the specification of several\n",
    "    key parameters when creating any environment, including the requested\n",
    "    environment and network classes, and the inputs needed to make these\n",
    "    classes generalizable to networks of varying sizes and shapes, and well as\n",
    "    varying forms of control.\n",
    "    \n",
    "    This method can also be used to recreate the environment a policy was\n",
    "    trained on and assess it performance, or a modified form of the previous\n",
    "    environment may be used to profile the performance of the policy on other\n",
    "    types of networks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : dict\n",
    "        MARL_routing-related parameters, consisting of the following keys:\n",
    "         - env_name: name of the flow environment the experiment is running on\n",
    "         - network: name of the network class the experiment uses\n",
    "    version : int, optional\n",
    "        environment version number\n",
    "    render : bool, optional\n",
    "        specifies whether to use the gui during execution. This overrides\n",
    "        the render attribute in SumoParams\n",
    "    Returns\n",
    "    -------\n",
    "    function\n",
    "        method that calls OpenAI gym's register method and make method\n",
    "    str\n",
    "        name of the created gym environment\n",
    "    \"\"\"\n",
    "    env_name = params[\"env_name\"] + '-v{}'.format(version)\n",
    "    \n",
    "    # TODO: associate network name with the network class! \n",
    "#     module = __import__(\"flow.networks\", fromlist=[params[\"network\"]])\n",
    "#     network_class = getattr(module, params[\"network\"])\n",
    "\n",
    "\n",
    "    def create_env(*_):\n",
    "        network = network_class()\n",
    "\n",
    "        # check if the environment is a single or multiagent environment, and\n",
    "        # get the right address accordingly\n",
    "        # This is needed for entry_point. \n",
    "        # Entry point: The Python entrypoint of the environment class (e.g. module.name:Class)\n",
    "        \n",
    "        if params['env_name'] in single_agent_envs:\n",
    "            env_loc = 'flow.envs'\n",
    "        else:\n",
    "            env_loc = 'flow.envs.multiagent'\n",
    "\n",
    "        try:\n",
    "            register(\n",
    "                id=env_name,\n",
    "                entry_point=env_loc + ':{}'.format(params[\"env_name\"]),\n",
    "                kwargs={\n",
    "                    \"env_params\": env_params,\n",
    "                    \"network\": network,\n",
    "                })\n",
    "        except Exception:\n",
    "            pass\n",
    "        return gym.envs.make(env_name)\n",
    "\n",
    "    return create_env, env_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Configurations\n",
    "\n",
    "EXP_NUM = 0\n",
    "\n",
    "# time horizon of a single rollout\n",
    "HORIZON = 1\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 1\n",
    "# number of parallel workers\n",
    "N_CPUS = 2\n",
    "        \n",
    "# Params for experiment\n",
    "routing_params = {\"exp_tag\": ,\n",
    "                  \"env_name\": ,\n",
    "                  \"network\": }\n",
    "\n",
    "def run_model(num_cpus=1, rollout_size=1, num_steps=10000):\n",
    "    \"\"\"Run the model for num_steps if provided. The total rollout length is rollout_size.\"\"\"\n",
    "    if num_cpus == 1:\n",
    "        constructor = env_constructor(params=routing_params, version=0)()\n",
    "        env = DummyVecEnv([lambda: constructor])  # The algorithms require a vectorized environment to run\n",
    "    else:\n",
    "        env = SubprocVecEnv([env_constructor(params=routing_params, version=i) for i in range(num_cpus)])\n",
    "\n",
    "    model = PPO2('MlpPolicy', env, verbose=1, n_steps=rollout_size)\n",
    "    model.learn(total_timesteps=num_steps)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestRunModel(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        pass\n",
    "    \n",
    "    def testPipelineWork(self):\n",
    "        pass\n",
    "    \n",
    "    def testModelIsWhatIExpectItToBe:\n",
    "        pass\n",
    "    \n",
    "    def testEnvConstructorCreatesHowIExpectTheEnvToBeMade:\n",
    "        pass\n",
    "    \n",
    "    def testAnEnvIsSuccessfullyRegisteredInGym:\n",
    "        pass\n",
    "    \n",
    "    def testAnyOtherFunctionalityThatsSpecificToStableBaselines:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Add this as the last thing to do for stable baseline conversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# MDP: (All characteristics of this MDP is given in W. Krichene's Paper \n",
    "#                            -- \"Learning Nash Equilibria in Congestion Games\")\n",
    "#\n",
    "#  - Observations/States: Each player will observe the cost (travel time) on all of the paths (according to paper)\n",
    "#                         If the player only observes the loss she incurs then it becomes a multiarmed bandit \n",
    "#                         setting.\n",
    "#  - Actions: Each player will choose a path, using a randomized/mixed strategy. \n",
    "#             This means we have a stochastic policy.\n",
    "#  - Reward: The *cost* of each player will be the travel time that they've incurred on their path. T\n",
    "#            Each player wants to minimize their travel time. For reward, we can maximize the negative cost.\n",
    "#  - Model of the environment: We don't have one in this case\n",
    "#\n",
    "###\n",
    "\n",
    "# Test 1: Test that reset() returns an initial observation. \n",
    "#         The initial observation should be:\n",
    "#                \"ABD\": 3\n",
    "#                \"ACD\": 3\n",
    "#                \"ABCD\": 2.25\n",
    "class TestBraessEnv(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        configs = {}\n",
    "        self.env = BraessEnv(configs)\n",
    "#         Representing 1 flow of .01 taking path \"ABD\"\n",
    "#         flow = {\"ABD\": 0.01,\n",
    "#                 \"ACD\": 0,\n",
    "#                 \"ABCD\": 0}\n",
    "        self.action = {\"population_1\": np.array([0.01, 0, 0])}  \n",
    "    \n",
    "#         flows = {\n",
    "#             \"ABD\": 0,\n",
    "#             \"ACD\": 0,\n",
    "#             \"ABCD\": 0.01\n",
    "#         } # Flow of 0.01 taking \"ABCD\"\n",
    "        self.action2 = {\"population_1\": np.array([0, 0, 0.01])}\n",
    "        \n",
    "    def testFormatOfObservationsandRewardsInStepandReset(self):\n",
    "        initial = self.env.reset()\n",
    "        items = list(initial.items())\n",
    "        self.assertEqual(len(items), 1)\n",
    "        \n",
    "        agent, obs = items[0]\n",
    "        \n",
    "        # Seems like the \"preprocessor\" in the reset function turns the dictionary into an numpy array\n",
    "        # Just check if it has 3 values associated with 3 paths\n",
    "        self.assertEqual(len(obs), 3)\n",
    "        \n",
    "        next_obs, reward, done, info = self.env.step(self.action)\n",
    "        \n",
    "        obs_items = list(next_obs.items())\n",
    "        reward_items = list(reward.items())\n",
    "        self.assertEqual(len(obs_items), 1)\n",
    "        self.assertEqual(len(reward_items), 1)\n",
    "        \n",
    "        actual_obs = obs_items[0][1]\n",
    "        actual_reward = reward_items[0][1]\n",
    "        \n",
    "        self.assertEqual(len(actual_obs), 3)\n",
    "        self.assertTrue(isinstance(actual_reward, float))\n",
    "        self.assertTrue(done[\"__all__\"])\n",
    "        \n",
    "        \n",
    "    def testResetReturnsObservation(self):\n",
    "        initial = self.env.reset()       \n",
    "        obs = list(initial.values())[0]\n",
    "        \n",
    "#         dict_expected = {\n",
    "#             \"ABD\": 3,\n",
    "#             \"ACD\": 3,\n",
    "#             \"ABCD\": 2.25\n",
    "#         }\n",
    "        actual_expected = np.array([3, 3, 2.25])\n",
    "        np.testing.assert_array_equal(obs, actual_expected)\n",
    "        \n",
    "    def testStepReturnsCorrectInformation(self):\n",
    "        # Test that step returns correct next observations, reward, and termination signal\n",
    "        env = self.env\n",
    "        env.reset()\n",
    "\n",
    "        next_obs, reward, done, _ = env.step(self.action)\n",
    "        # dict_obs = {\n",
    "        #     \"ABD\": 3.01,\n",
    "        #     \"ACD\": 3,\n",
    "        #     \"ABCD\": 2.26\n",
    "        # }\n",
    "        actual_expected_obs = [3.01, 3, 2.26]\n",
    "        \n",
    "        # travel_times = {\n",
    "        #     \"ABD\": 3.01,\n",
    "        #     \"ACD\": 3,\n",
    "        #     \"ABCD\": 2.26\n",
    "        # }\n",
    "        flow_dist = np.array([0.01, 0, 0])\n",
    "        path_rewards = np.array([-3.01, -3, -2.26]) # Rewards are negative of travel times\n",
    "        expected_reward = np.dot(flow_dist, path_rewards)\n",
    "        \n",
    "        np.testing.assert_array_equal(actual_expected_obs, list(next_obs.values())[0])\n",
    "        self.assertEqual(expected_reward, list(reward.values())[0])\n",
    "        self.assertTrue(done[\"__all__\"])\n",
    "        \n",
    "    def testStepSavesNoPrevInfo(self):\n",
    "        env = self.env\n",
    "        env.reset()\n",
    "        env.step(self.action)\n",
    "    \n",
    "        next_obs, reward, done, _ = env.step(self.action2)\n",
    "        \n",
    "#         dict_obs = {\n",
    "#             \"ABD\": 3.01,\n",
    "#             \"ACD\": 3.01,\n",
    "#             \"ABCD\": 2.27\n",
    "#         }\n",
    "#         travel_times = {\n",
    "#             \"ABD\": 3.01,\n",
    "#             \"ACD\": 3.01,\n",
    "#             \"ABCD\": 2.27\n",
    "#         }\n",
    "        \n",
    "        expected_obs = np.array([3.01, 3.01, 2.27])\n",
    "        \n",
    "        flow_dist = np.array([0, 0, 0.01])\n",
    "        path_rewards = np.array([-3.01, -3.01, -2.27])\n",
    "        expected_reward = np.dot(flow_dist, path_rewards)\n",
    "        \n",
    "        np.testing.assert_array_equal(expected_obs, list(next_obs.values())[0])\n",
    "        self.assertEqual(expected_reward, list(reward.values())[0])\n",
    "        self.assertTrue(done[\"__all__\"])\n",
    "        \n",
    "    def testEnvDoesNotKeepActionsFromPreviousRun(self):\n",
    "#         env = self.env\n",
    "#         # Episode 1\n",
    "#         env.reset()\n",
    "#         env.step(self.action)\n",
    "        \n",
    "#         # Episode 2\n",
    "#         env.reset()\n",
    "# #         flows = {\n",
    "# #             \"ABD\": 0,\n",
    "# #             \"ACD\": 0,\n",
    "# #             \"ABCD\": 0.01\n",
    "# #         } # Flow of 0.01 taking \"ABCD\"\n",
    "# # .     {\"population_1\": np.array([0, 0, 0.01])}\n",
    "    \n",
    "#         env.step(self.action2)\n",
    "        \n",
    "#         # Check\n",
    "#         self.fail()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.265s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABCD': 0, 'ABD': 0, 'ACD': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = BraessNetwork()\n",
    "{route: 0 for route in n.routes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is what was given: {'ABCD': 3.75, 'ABD': 3.75, 'ACD': 3.75}\n",
      "This is what I expect: {'ABCD': 3.75, 'ABD': 3.75, 'ACD': 3.75}\n",
      "---\n",
      "This is what was given: {'ABCD': 3.25, 'ABD': 3.5, 'ACD': 3.5}\n",
      "This is what I expect: {'ABCD': 3.25, 'ABD': 3.5, 'ACD': 3.5}\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "network = BraessNetwork()\n",
    "\n",
    "# Test 1 for calculate ttime\n",
    "#\n",
    "#                  B\n",
    "#                / | \\                   \n",
    "#             /    |    \\\n",
    "#          A       |       D\n",
    "#             \\    |    /\n",
    "#                \\ | /\n",
    "#                  C\n",
    "#\n",
    "# Out of 100 cars, we will do: \n",
    "#     ABD = 25; \n",
    "#     ACD = 25; \n",
    "#     ABCD = 50 \n",
    "# \n",
    "# The travel time on each path should result as:\n",
    "#     ABD = 3.75 (units)\n",
    "#     ACD = 3.75 (units)\n",
    "#     ABCD = 3.75 (units)\n",
    "#\n",
    "flows = {\n",
    "    \"ABD\": 0.25,\n",
    "    \"ACD\": 0.25,\n",
    "    \"ABCD\": 0.50\n",
    "}\n",
    "expect1 = {\n",
    "    \"ABD\": 3.75,\n",
    "    \"ACD\": 3.75,\n",
    "    \"ABCD\": 3.75\n",
    "}\n",
    "times = network.calculate_ttime(flows)\n",
    "print(\"This is what was given: \" + str(times))\n",
    "print(\"This is what I expect: \" + str(expect1))\n",
    "print(\"---\")\n",
    "\n",
    "# Test 2 for calculate ttime\n",
    "# Out of 100 cars, we will do: \n",
    "#     ABD = 50; \n",
    "#     ACD = 50; \n",
    "#     ABCD = 0 \n",
    "# \n",
    "# The travel time on each path should result as:\n",
    "#     ABD = 3.5 (units)\n",
    "#     ACD = 3.5 (units)\n",
    "#     ABCD = 3.25 (units)  - Even though no one's using this path\n",
    "flows = {\n",
    "    \"ABD\": 0.50,\n",
    "    \"ACD\": 0.50,\n",
    "    \"ABCD\": 0\n",
    "}\n",
    "expect2 = {\n",
    "    \"ABD\": 3.5,\n",
    "    \"ACD\": 3.5,\n",
    "    \"ABCD\": 3.25\n",
    "}\n",
    "times = network.calculate_ttime(flows)\n",
    "print(\"This is what was given: \" + str(times))\n",
    "print(\"This is what I expect: \" + str(expect2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to run experiments\n",
    "gamma = 0.5\n",
    "single_pop_network = BraessEnv({})\n",
    "config = {\"gamma\": gamma}\n",
    "policy_graphs = {\n",
    "    'population': (PPOPolicyGraph, single_pop_network.observation_space, single_pop_network.action_space, config)\n",
    "}\n",
    "\n",
    "BRAESS_CONFIG = {\n",
    "    # === Environment ===\n",
    "    # Discount factor of the MDP\n",
    "    \"gamma\": gamma, # Ask Yiling\n",
    "    # Number of steps after which the episode is forced to terminate. Defaults\n",
    "    # to `env.spec.max_episode_steps` (if present) for Gym envs.\n",
    "    \"horizon\": 1,\n",
    "    # Calculate rewards but don't reset the environment when the horizon is\n",
    "    # hit. This allows value estimation and RNN state to span across logical\n",
    "    # episodes denoted by horizon. This only has an effect if horizon != inf.\n",
    "    \"soft_horizon\": True,\n",
    "    # Don't set 'done' at the end of the episode. Note that you still need to\n",
    "    # set this if soft_horizon=True, unless your env is actually running\n",
    "    # forever without returning done=True.\n",
    "    \"no_done_at_end\": True,\n",
    "    # The default learning rate\n",
    "    \"lr\": 0.0001, # Ask Yiling\n",
    "\n",
    "    # === Evaluation ===\n",
    "    # Evaluate with every `evaluation_interval` training iterations.\n",
    "    # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "    # Note that evaluation is currently not parallelized, and that for Ape-X\n",
    "    # metrics are already only reported for the lowest epsilon workers.\n",
    "    \"evaluation_interval\": None,\n",
    "    # Number of episodes to run per evaluation period.\n",
    "    \"evaluation_num_episodes\": 1,\n",
    "\n",
    "    # === Multiagent ===\n",
    "    \"multiagent\": {\n",
    "        'policy_graphs': policy_graphs,\n",
    "        # Function mapping agent ids to policy ids.\n",
    "        \"policy_mapping_fn\": tune.function(lambda agent_id: 'population')\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-09-12_01-22-55_2512/logs.\n",
      "Waiting for redis server at 127.0.0.1:54865 to respond...\n",
      "Waiting for redis server at 127.0.0.1:26633 to respond...\n",
      "Starting the Plasma object store with 6.871947672999999 GB memory using /tmp.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=4f93041440b64501ac927342e2dadf9302cfc7dbd9ce3e45\n",
      "======================================================================\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 12.1/17.2 GB\n",
      "\n",
      "Created LogSyncer for /Users/mtgibson/ray_results/route-DQN/PPO_multi_routing_0_2019-09-12_01-22-55t47ksgcr -> \n",
      "WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n",
      "WARNING: Falling back to serializing objects of type <class 'mtrand.RandomState'> by using pickle. This may be inefficient.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 12.1/17.2 GB\n",
      "Result logdir: /Users/mtgibson/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - PPO_multi_routing_0:\tRUNNING\n",
      "\n",
      "Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/marl_routing/lib/python3.5/site-packages/ray/tune/trial_runner.py\", line 261, in _process_events\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/anaconda3/envs/marl_routing/lib/python3.5/site-packages/ray/tune/ray_trial_executor.py\", line 211, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/anaconda3/envs/marl_routing/lib/python3.5/site-packages/ray/worker.py\", line 2386, in get\n",
      "    raise value\n",
      "ray.worker.RayTaskError: \u001b[36mray_PPOAgent:train()\u001b[39m (pid=2533, host=C02X23BZJHD3)\n",
      "  File \"/anaconda3/envs/marl_routing/lib/python3.5/site-packages/ray/rllib/agents/agent.py\", line 244, in __init__\n",
      "    Trainable.__init__(self, config, logger_creator)\n",
      "  File \"/anaconda3/envs/marl_routing/lib/python3.5/site-packages/ray/tune/trainable.py\", line 87, in __init__\n",
      "    self._setup(copy.deepcopy(self.config))\n",
      "  File \"/anaconda3/envs/marl_routing/lib/python3.5/site-packages/ray/rllib/agents/agent.py\", line 304, in _setup\n",
      "    self._allow_unknown_subkeys)\n",
      "  File \"/anaconda3/envs/marl_routing/lib/python3.5/site-packages/ray/rllib/utils/__init__.py\", line 35, in deep_update\n",
      "    raise Exception(\"Unknown config parameter `{}` \".format(k))\n",
      "Exception: Unknown config parameter `soft_horizon` \n",
      "\n",
      "Worker ip unknown, skipping log sync for /Users/mtgibson/ray_results/route-DQN/PPO_multi_routing_0_2019-09-12_01-22-55t47ksgcr\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 13.4/17.2 GB\n",
      "Result logdir: /Users/mtgibson/ray_results/route-DQN\n",
      "ERROR trials:\n",
      " - PPO_multi_routing_0:\tERROR, 1 failures: /Users/mtgibson/ray_results/route-DQN/PPO_multi_routing_0_2019-09-12_01-22-55t47ksgcr/error_2019-09-12_01-23-04.txt\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 13.4/17.2 GB\n",
      "Result logdir: /Users/mtgibson/ray_results/route-DQN\n",
      "ERROR trials:\n",
      " - PPO_multi_routing_0:\tERROR, 1 failures: /Users/mtgibson/ray_results/route-DQN/PPO_multi_routing_0_2019-09-12_01-22-55t47ksgcr/error_2019-09-12_01-23-04.txt\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_multi_routing_0])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-11a95f958c76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     }\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mrun_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/marl_routing/lib/python3.5/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(experiments, search_alg, scheduler, with_server, server_port, verbose, queue_trials, trial_executor, raise_on_failed_trial)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_multi_routing_0])"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env_creator_name = 'multi_routing'\n",
    "    register_env(env_creator_name, lambda config: BraessEnv(config))\n",
    "    ray.init()\n",
    "    experiments = {\n",
    "        'route-DQN': {\n",
    "            'run': 'PPO',\n",
    "            'env': 'multi_routing',\n",
    "            'stop': {\n",
    "                'training_iteration': 5\n",
    "            },\n",
    "            'config': BRAESS_CONFIG\n",
    "        },\n",
    "        # put additional experiments to run concurrently here\n",
    "    }\n",
    "    \n",
    "    run_experiments(experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots \n",
    "\n",
    "The following code plots the RL results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "file = open(filename, 'r')\n",
    "j = 0\n",
    "# we want to plot the evolution of the flow distribution, of the travel time, and of the reward/average travel time\n",
    "Actions_plot = np.array([[0, 0, 0]]) # flow path\n",
    "Reward_plot = np.array([0]) # average travel time/cost or reward\n",
    "Travel_time_plot = np.array([[0, 0, 0]]) # travel time on each path\n",
    "while(True):\n",
    "    j = j+1\n",
    "    data = file.readline()\n",
    "#     if j%100 != 1:\n",
    "#         continue\n",
    "    try:\n",
    "#         action_dict = ast.literal_eval(\"{\" + actions.split('{')[1].split('}')[0]+ \"}\")\n",
    "#         reward_dict = ast.literal_eval(\"{\" + rewards.split('{')[1].split('}')[0]+ \"}\")\n",
    "        \n",
    "        data = data.split(';')\n",
    "        \n",
    "    \n",
    "#         network = Networks.network(network_name, nb_veh)\n",
    "#         travel_time, marginal_cost, rew_dict = get_tt_mc(action_dict, network, 0)\n",
    "        \n",
    "#         actions_np = np.fromiter(action_dict.values(), dtype=int)\n",
    "#         Actions_plot = np.append(Actions_plot, [actions_np], axis=0)\n",
    "#         rewards_np = np.fromiter(reward_dict.values(), dtype=float)\n",
    "#         Reward_plot = np.append(Reward_plot, [rewards_np], axis=0)\n",
    "#         travel_time_np = np.fromiter(travel_time.values(), dtype=float)\n",
    "#         Travel_time_plot = np.append(Travel_time_plot, [travel_time_np], axis=0)\n",
    "#         if(j==1):\n",
    "#             print(\"------ First iteration ------\")\n",
    "#             print(\"Path choice: \" + str(action_dict))\n",
    "#             print(\"Reward ray: \" + str(reward_dict))\n",
    "#             print(\"Travel time paths: \" + str({\"path \" + str(i): network.travel_time(i) for i in range(3)}))\n",
    "#             print(\"Travel time cars: \" + str(travel_time))\n",
    "#             print(\"Marginal cost: \" + str(marginal_cost))\n",
    "#             print(\"Reward network: \" + str(rew_dict))\n",
    "    except:\n",
    "        print()\n",
    "        print(\"------ Last iteration ------\")\n",
    "        print(\"Path choice: \" + str(action_dict))\n",
    "        print(\"Reward ray: \" + str(reward_dict))\n",
    "        print(\"Travel time paths: \" + str({\"path \" + str(i): network.travel_time(i) for i in range(3)}))\n",
    "        print(\"Travel time cars: \" + str(travel_time))\n",
    "        print(\"Marginal cost: \" + str(marginal_cost))\n",
    "        print(\"Reward network: \" + str(rew_dict))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
