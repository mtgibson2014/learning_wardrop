{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BraessEnv(Env):\n",
    "    \"\"\"Traffic Environment that uses the Braess's network. \n",
    "       See https://github.com/openai/gym/blob/master/gym/core.py for more details.\n",
    "    \"\"\"\n",
    "    action_space = \"CHANGE THIS\" # This will be a box\n",
    "    observation_space = \"CHANGE THIS\" # This will be a box\n",
    "    reward_range = \"CHANGE THIS\"  # (-float('inf'), 0)\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics. \n",
    "        \n",
    "        Env \n",
    "            (1) takes in routing distribution - comes from the action, \n",
    "            (2) calculate travel times for each path given flow on each path, \n",
    "            (3) return the reward which is the negative of the travel time.\n",
    "            \n",
    "        Args:\n",
    "            action (array): Distribution of flow for each path for o-d pair. First element corresponds to \n",
    "                            distribution for path 1, second element is for path 2, etc.\n",
    "        \n",
    "        Returns:\n",
    "            next_observation (array): the travel times determined for each path\n",
    "            reward (float): -1*travel_time_of_agent\n",
    "            done (boolean): _\n",
    "            info (dict): other information needed - don't really need now though\n",
    "        \"\"\"\n",
    "        return {}, float(\"inf\"), True, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the state of the environment and returns an initial observation.\n",
    "        \n",
    "        For the initial observation: Make an array of 3 elements corresponding to 3 paths in\n",
    "        the Braess network. It should have the format ---\n",
    "        \n",
    "        state = [<traveltime_ABD>, <traveltime_ACD>, <traveltime_ABCD>] = [2, 2, 0.25]\n",
    "        \"\"\"\n",
    "        self.state = []\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def render(self):\n",
    "        return\n",
    "    \n",
    "    def close(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoLearningAgent(Env):\n",
    "    \"\"\"Agent that chooses some random action to do.\"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.__action_space = action_space\n",
    "        return\n",
    "    \n",
    "    def take_first_action(self, obs):\n",
    "        return self.__action_space.sample()\n",
    "    \n",
    "    def take_action(self, new_obs, old_reward):\n",
    "        return self.__action_space.sample()\n",
    "    \n",
    "    def update_policy():\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
