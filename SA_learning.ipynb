{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import *\n",
    "import unittest\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ece10dbbc9ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBraessEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"Traffic Environment that uses the Braess's network. \n\u001b[1;32m      3\u001b[0m        \u001b[0mSee\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mgithub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mopenai\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0maction_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"CHANGE THIS\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Env' is not defined"
     ]
    }
   ],
   "source": [
    "class BraessEnv(Env):\n",
    "    \"\"\"Traffic Environment that uses the Braess's network. \n",
    "       See https://github.com/openai/gym/blob/master/gym/core.py for more details.\n",
    "    \"\"\"\n",
    "    action_space = \"CHANGE THIS\" # This will be a box\n",
    "    observation_space = \"CHANGE THIS\" # This will be a box\n",
    "    reward_range = \"CHANGE THIS\"  # (-float('inf'), 0)\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics. \n",
    "        \n",
    "        Env \n",
    "            (1) takes in routing distribution - comes from the action, \n",
    "            (2) calculate travel times for each path given flow on each path, \n",
    "            (3) return the reward which is the negative of the travel time.\n",
    "            \n",
    "        Args:\n",
    "            action (array): Distribution of flow for each path for o-d pair. First element corresponds to \n",
    "                            distribution for path 1, second element is for path 2, etc.\n",
    "        \n",
    "        Returns:\n",
    "            next_observation (array): the travel times determined for each path\n",
    "            reward (float): -1*travel_time_of_agent\n",
    "            done (boolean): _\n",
    "            info (dict): other information needed - don't really need now though\n",
    "        \"\"\"\n",
    "        return {}, float(\"inf\"), True, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the state of the environment and returns an initial observation.\n",
    "        \n",
    "        For the initial observation: Make an array of 3 elements corresponding to 3 paths in\n",
    "        the Braess network. It should have the format ---\n",
    "        \n",
    "        state = [<traveltime_ABD>, <traveltime_ACD>, <traveltime_ABCD>] = [2, 2, 0.25]\n",
    "        \"\"\"\n",
    "        self.state = []\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def render(self):\n",
    "        return\n",
    "    \n",
    "    def close(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BraessNetwork(object):\n",
    "    \"\"\"Stores the cost for all links. Handles calculating the cost of a path given action\n",
    "       of every car.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.__links = {\n",
    "            \"AB\": lambda f: 1 + (f/100),\n",
    "            \"AC\": lambda _: 2,\n",
    "            \"BD\": lambda _: 2,\n",
    "            \"CD\": lambda f: 1 + (f/100),\n",
    "            \"BC\": lambda _: 0.25\n",
    "        } # Dictionary of links and their congestion functions\n",
    "        self.__paths = {\n",
    "            \"ABD\": (\"AB\", \"BD\"),\n",
    "            \"ACD\": (\"AC\", \"CD\"),\n",
    "            \"ABCD\": (\"AB\", \"BC\", \"CD\")\n",
    "        } # Dictionaries of paths to links\n",
    "        return \n",
    "    \n",
    "    def paths(self):\n",
    "        \"\"\"Gives a list of all possible paths in the network to the environment. \n",
    "           The environment could then assign an action number to each path. \n",
    "        \"\"\"\n",
    "        return (\"ABD\", \"ACD\", \"ABCD\")\n",
    "    \n",
    "    def calculate_ttime(self, flows):\n",
    "        \"\"\"Given a dictionary of paths and flows, this function returns a dictionary of \n",
    "           paths and travel time (secs), a.k.a ttime.\n",
    "           \n",
    "           Returns: \n",
    "               travel_times (dictionary): A dictionary of paths to their travel times\n",
    "        \"\"\"\n",
    "        congestion = {}\n",
    "        for path in flows:\n",
    "            links = self.__paths[path]\n",
    "            for link in links:\n",
    "                if link not in congestion:\n",
    "                    congestion[link] = 0\n",
    "                congestion[link] += flows[path]\n",
    "        \n",
    "        t_time = {}\n",
    "        for path in flows:\n",
    "            total_time = 0\n",
    "            # Calculate travel time of path by adding the congestion time of every \n",
    "            # link in that path\n",
    "            links = self.__paths[path]\n",
    "            for link in links:\n",
    "                t_time_func = self.__links[link]\n",
    "                total_time += t_time_func(congestion[link])\n",
    "            t_time[path] = total_time\n",
    "        \n",
    "        return t_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoLearningAgent(Env):\n",
    "    \"\"\"Agent that chooses some random action to do.\"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.__action_space = action_space\n",
    "        return\n",
    "    \n",
    "    def take_first_action(self, obs):\n",
    "        return self.__action_space.sample()\n",
    "    \n",
    "    def take_action(self, new_obs, old_reward):\n",
    "        return self.__action_space.sample()\n",
    "    \n",
    "    def update_policy():\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 100    # Should change this depending on how many times you want to run the routing game\n",
    "\n",
    "def learn():\n",
    "    \"\"\"Executes our learning framework.\"\"\" \n",
    "    env = BraessEnv()\n",
    "    agent = SimpleAgent(env.action_space)\n",
    "    obs = env.reset()\n",
    "    for i in range(num_iter):\n",
    "        if i == 0:\n",
    "            action = agent.take_first_action(obs)\n",
    "        else:\n",
    "            action = agent.take_action(obs, rw)\n",
    "        obs, rw, _, _ = env.step(action)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABD': 3.75, 'ACD': 3.75, 'ABCD': 3.75}\n",
      "{'ABD': 3.5, 'ACD': 3.5, 'ABCD': 3.25}\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "network = BraessNetwork()\n",
    "\n",
    "# Test 1 for calculate ttime\n",
    "#\n",
    "#                  B\n",
    "#                / | \\                   \n",
    "#             /    |    \\\n",
    "#          A       |       D\n",
    "#             \\    |    /\n",
    "#                \\ | /\n",
    "#                  C\n",
    "#\n",
    "# Out of 100 cars, we will do: \n",
    "#     ABD = 25; \n",
    "#     ACD = 25; \n",
    "#     ABCD = 50 \n",
    "# \n",
    "# The travel time on each path should result as:\n",
    "#     ABD = 3.75 (units)\n",
    "#     ACD = 3.75 (units)\n",
    "#     ABCD = 3.75 (units)\n",
    "#\n",
    "flows = {\n",
    "    \"ABD\": 25,\n",
    "    \"ACD\": 25,\n",
    "    \"ABCD\": 50\n",
    "}\n",
    "times = network.calculate_ttime(flows)\n",
    "print(times)\n",
    "\n",
    "\n",
    "# Test 2 for calculate ttime\n",
    "# Out of 100 cars, we will do: \n",
    "#     ABD = 50; \n",
    "#     ACD = 50; \n",
    "#     ABCD = 0 \n",
    "# \n",
    "# The travel time on each path should result as:\n",
    "#     ABD = 3.5 (units)\n",
    "#     ACD = 3.5 (units)\n",
    "#     ABCD = 3.25 (units)  - Even though no one's using this path\n",
    "flows = {\n",
    "    \"ABD\": 50,\n",
    "    \"ACD\": 50,\n",
    "    \"ABCD\": 0\n",
    "}\n",
    "times = network.calculate_ttime(flows)\n",
    "print(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# MDP: (All characteristics of this MDP is given in W. Krichene's Paper \n",
    "#                            -- \"Learning Nash Equilibria in Congestion Games\")\n",
    "#\n",
    "#  - Observations/States: Each player will observe the cost (travel time) on all of the paths (according to paper)\n",
    "#                         If the player only observes the loss she incurs then it becomes a multiarmed bandit \n",
    "#                         setting.\n",
    "#  - Actions: Each player will choose a path, using a randomized/mixed strategy. \n",
    "#             This means we have a stochastic policy.\n",
    "#  - Reward: The *cost* of each player will be the travel time that they've incurred on their path. T\n",
    "#            Each player wants to minimize their travel time. For reward, we can maximize the negative cost.\n",
    "#  - Model of the environment: We don't have one in this case\n",
    "#\n",
    "###\n",
    "\n",
    "# Test 1: Test that reset() returns an initial observation. \n",
    "#         The initial observation should be:\n",
    "#                \"ABD\": 3\n",
    "#                \"ACD\": 3\n",
    "#                \"ABCD\": 2.25\n",
    "class TestBraessEnv(unittest.TestCase):\n",
    "    def testResetReturnsObservation(self):\n",
    "        env = BraessEnv()\n",
    "        init = env.reset()\n",
    "        expected = {\n",
    "            \"ABD\": 3,\n",
    "            \"ACD\": 3,\n",
    "            \"ABCD\": 2.25\n",
    "        }\n",
    "        self.assertDictEqual(expected, init)\n",
    "        \n",
    "    def testStepReturnsCorrectInformation(self):\n",
    "        # Test that step returns correct next observations, reward, and termination signal\n",
    "        env = BraessEnv()\n",
    "        _ = env.reset()\n",
    "        # Note: Currently, it's expected that the single car will specify a number that corresponds to a route.\n",
    "        #       This will have to change when we switch to the nonatomic setting.\n",
    "        action = 1  # Representing path \"ABD\"\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        expected_obs = {\n",
    "            \"ABD\": 3.01,\n",
    "            \"ACD\": 3,\n",
    "            \"ABCD\": 2.25\n",
    "        }\n",
    "        expected_reward = -3.01\n",
    "        expected_done = True\n",
    "        \n",
    "        self.assertDictEqual(expected_obs, next_obs)\n",
    "        self.assertEqual(expected_reward, reward)\n",
    "        self.assertEqual(expected_done, done)\n",
    "        \n",
    "    def testStepSavesNoPrevInfo(self):\n",
    "        env = BraessEnv()\n",
    "        env.reset()\n",
    "        action = 1  # Representing path \"ABD\"\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        action = 3\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        expected_obs = {\n",
    "            \"ABD\": 3,\n",
    "            \"ACD\": 3,\n",
    "            \"ABCD\": 2.27\n",
    "        }\n",
    "        expected_reward = -2.27\n",
    "        expected_done = True\n",
    "        \n",
    "        self.assertDictEqual(expected_obs, next_obs)\n",
    "        self.assertEqual(expected_reward, reward)\n",
    "        self.assertEqual(expected_done, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FFF\n",
      "======================================================================\n",
      "FAIL: testResetReturnsObservation (__main__.TestBraessEnv)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-33-78c470cd245e>\", line 30, in testResetReturnsObservation\n",
      "    self.assertDictEqual(expected, init)\n",
      "AssertionError: {'ABD': 3, 'ACD': 3, 'ABCD': 2.25} != {}\n",
      "- {'ABCD': 2.25, 'ABD': 3, 'ACD': 3}\n",
      "+ {}\n",
      "\n",
      "======================================================================\n",
      "FAIL: testStepReturnsCorrectInformation (__main__.TestBraessEnv)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-33-78c470cd245e>\", line 48, in testStepReturnsCorrectInformation\n",
      "    self.assertDictEqual(expected_obs, next_obs)\n",
      "AssertionError: {'ABD': 3.01, 'ACD': 3, 'ABCD': 2.25} != {}\n",
      "- {'ABCD': 2.25, 'ABD': 3.01, 'ACD': 3}\n",
      "+ {}\n",
      "\n",
      "======================================================================\n",
      "FAIL: testStepSavesNoPrevInfo (__main__.TestBraessEnv)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-33-78c470cd245e>\", line 68, in testStepSavesNoPrevInfo\n",
      "    self.assertDictEqual(expected_obs, next_obs)\n",
      "AssertionError: {'ABD': 3, 'ACD': 3, 'ABCD': 2.27} != {}\n",
      "- {'ABCD': 2.27, 'ABD': 3, 'ACD': 3}\n",
      "+ {}\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.003s\n",
      "\n",
      "FAILED (failures=3)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
