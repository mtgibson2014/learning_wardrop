{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/marl_routing/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from gym import Env, spaces\n",
    "import unittest\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import collections\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "from gym.envs.registration import EnvSpec, register\n",
    "\n",
    "import os\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/Users/mtgibson/learning_wardrop/graph_results.txt\"\n",
    "\n",
    "filename2 = \"/Users/mtgibson/learning_wardrop/model_results.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BraessEnv(gym.Env):\n",
    "    \"\"\"Traffic Environment that uses the Braess's network. \n",
    "       See https://github.com/openai/gym/blob/master/gym/core.py for more details.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params=None):\n",
    "        # Make the Braess's network\n",
    "        self.network = BraessNetwork()\n",
    "        self.routes = self.network.routes\n",
    "        \n",
    "        # Observation space contains each route and travel times\n",
    "        self.num_routes = len(self.routes)\n",
    "        self.observation_space = spaces.Box(low=0,high=float('+inf'),shape=(3,),dtype=np.float32)\n",
    "        \n",
    "        # Action space contains each route and flow distribution of population (decimal)\n",
    "        self.action_space = action_spaces = spaces.Box(low=0,high=1,shape=(3,),dtype=np.float32)\n",
    "        \n",
    "        self.reward_range = (-float('inf'), 0)\n",
    "        \n",
    "        #Storage bins for data\n",
    "        self.__path_flows = []\n",
    "        self.__travel_times = []\n",
    "        self.__avg_times = []\n",
    "        return\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        \"\"\"Run one timestep of the environment's dynamics. \n",
    "        \n",
    "        Env Step Procedure:\n",
    "            (1) takes in routing distribution - comes from the action, \n",
    "            (2) calculate travel times for each path given flow on each path, \n",
    "            (3) return the reward which is the negative of the travel time.\n",
    "            \n",
    "        Note: We will give a reward to each path and will return an array/dictionary \n",
    "              as the reward for the population.\n",
    "            \n",
    "        Args:\n",
    "            action (dictionary): A dictionary where the key is a population and values are another dictionary where\n",
    "                                 the key is a path and the value is a flow distribution.\n",
    "                                 We assume there is only 1 agent and thus one o-d pair\n",
    "        \n",
    "        Returns:\n",
    "            next_observation (array): the travel times determined for each path\n",
    "            reward (float): -1*travel_time_of_agent\n",
    "            done (boolean): _\n",
    "            info (dict): other information needed - don't really need now though\n",
    "        \"\"\"\n",
    "        obs_dict, rew_dict, done, info_dict = {}, {}, {}, {}\n",
    "        \n",
    "        \n",
    "        for agent, action in action_dict.items():\n",
    "            # Create action dictionary\n",
    "            action_dict = {}\n",
    "            for i in range(len(action)):\n",
    "                action_dict[self.routes[i]] = action[i]\n",
    "            \n",
    "            # Calculate the travel times and store flow distributions and travel times (Edit for Multi-agent)\n",
    "            travel_times_dict = self.network.calculate_ttime(action_dict)\n",
    "            self.__travel_times.append(travel_times_dict)\n",
    "            \n",
    "            #Transform dictionary into list\n",
    "            travel_times = []\n",
    "            for route in self.routes:\n",
    "                travel_times.append(travel_times_dict[route])\n",
    "            \n",
    "            # Calculate the reward for the population - mean (negative) travel time (Edit for MA w/ different routes)\n",
    "            reward = np.dot(np.array(action), \n",
    "                            -1*np.array(travel_times))\n",
    "            \n",
    "            obs_dict[agent] = np.array(travel_times) \n",
    "            rew_dict[agent] = reward\n",
    "            done[agent] = True\n",
    "            info_dict[agent] = {}\n",
    "            \n",
    "            # Add data to the storage bins\n",
    "            self.add_data(action, travel_times, reward)\n",
    "                            \n",
    "        done[\"__all__\"] = True\n",
    "        return obs_dict, rew_dict, done, info_dict\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the state of the environment and returns an initial observation.\n",
    "        \n",
    "        For the initial observation: Make an array of 3 elements corresponding to 3 paths in\n",
    "        the Braess network. It should have the format ---\n",
    "        \n",
    "        state = [<traveltime_ABD>, <traveltime_ACD>, <traveltime_ABCD>] = [2, 2, 0.25]\n",
    "        \"\"\"\n",
    "        # Calculate initial travel times with 0 flow on the network\n",
    "        flows = {route: 0 for route in self.routes}\n",
    "        initial_dict = self.network.calculate_ttime(flows)\n",
    "        \n",
    "        # Turn initial observation to an array\n",
    "        t_0 = []\n",
    "        for route in self.routes:\n",
    "            t_0.append(initial_dict[route])\n",
    "        \n",
    "        # State will be a numpy array\n",
    "        self.state = {'population_1': np.array(t_0)}\n",
    "        \n",
    "        # Reset the data bins and insert first item\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "        self.file = open(filename, \"a+\")\n",
    "        self.file.write(str(t_0) + \"\\n\")\n",
    "        self.file.close()\n",
    "        \n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    def add_data(self, path_flow, travel_time, reward):\n",
    "        self.file = open(filename, \"a\")\n",
    "        self.file.write(str(path_flow) + ';' + str(travel_time) + ';' + str(reward) + '\\n')\n",
    "        self.file.close()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BraessNetwork(object):\n",
    "    \"\"\"Stores the cost for all links. Handles calculating the cost of a path given action\n",
    "       of every car.\n",
    "    \"\"\"\n",
    "    def __init__(self, params=None):\n",
    "        self.__links = {\n",
    "            \"AB\": lambda f: 1 + (f/100),\n",
    "            \"AC\": lambda _: 2,\n",
    "            \"BD\": lambda _: 2,\n",
    "            \"CD\": lambda f: 1 + (f/100),\n",
    "            \"BC\": lambda _: 0.25\n",
    "        } # Dictionary of links and their congestion functions\n",
    "        self.__paths = {\n",
    "            \"ABD\": (\"AB\", \"BD\"),\n",
    "            \"ACD\": (\"AC\", \"CD\"),\n",
    "            \"ABCD\": (\"AB\", \"BC\", \"CD\")\n",
    "        } # Dictionaries of paths to links\n",
    "        self.total_flow = 100  # 100 cars in total on this network\n",
    "        return \n",
    "    \n",
    "    @property\n",
    "    def routes(self):\n",
    "        \"\"\"Gives a list of all possible paths in the network to the environment. \n",
    "           The environment could then assign an action number to each path. \n",
    "        \"\"\"\n",
    "        return (\"ABD\", \"ACD\", \"ABCD\")\n",
    "    \n",
    "    def calculate_ttime(self, flows):\n",
    "        \"\"\"Given a dictionary of paths and flows, this function returns a dictionary of \n",
    "           paths and travel time (secs), a.k.a ttime.\n",
    "           \n",
    "           Arg:\n",
    "               flows (dictionary): A dictionare where the key correspond to a path in the network of one o-d pair\n",
    "                                   and the value corresponds to the flow on that path. Flow will be a float\n",
    "                                   between 0 and 1 represent the percent of flow. \n",
    "           \n",
    "           Returns: \n",
    "               travel_times (list): A list of travel times, order matters \n",
    "                                    --> according to the order in my list of paths.\n",
    "        \"\"\"\n",
    "        congestion = {}\n",
    "        for path in flows:\n",
    "            links = self.__paths[path]\n",
    "            for link in links:\n",
    "                if link not in congestion:\n",
    "                    congestion[link] = 0\n",
    "                congestion[link] += flows[path] * self.total_flow\n",
    "        \n",
    "        t_time = {}\n",
    "        for path in flows:\n",
    "            total_time = 0\n",
    "            # Calculate travel time of path by adding the congestion time of every \n",
    "            # link in that path\n",
    "            links = self.__paths[path]\n",
    "            for link in links:\n",
    "                t_time_func = self.__links[link]\n",
    "                total_time += t_time_func(congestion[link])\n",
    "            t_time[path] = total_time\n",
    "        \n",
    "        \n",
    "        return t_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network2(object):\n",
    "    \"\"\"Stores the cost for all links. Handles calculating the cost of a path given action\n",
    "       of every car.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.__links = {\n",
    "            \"01\": lambda f: f + 2., \n",
    "            \"04\": lambda f: f/2,\n",
    "            \"05\": lambda f: f,\n",
    "            \"51\": lambda f: f/3,\n",
    "            \"45\": lambda f: 3*f, \n",
    "            \"43\": lambda f: f, \n",
    "            \"24\": lambda _: 0.5,\n",
    "            \"23\": lambda f: f + 1.,\n",
    "            \"53\": lambda f: f/4\n",
    "        } # Dictionary of links and their congestion functions\n",
    "        self.__paths = {\n",
    "            \"01\": (\"01\",\"11\"),\n",
    "            \"051\": (\"05\", \"51\"),\n",
    "            \"0451\": (\"04\", \"45\", \"51\"),\n",
    "            \"23\": (\"23\",\"33\"),\n",
    "            \"243\": (\"24\",\"43\"),\n",
    "            \"2453\": (\"24\",\"45\",\"53\")\n",
    "        } # Dictionaries of paths to links\n",
    "        return \n",
    "    \n",
    "    def paths(self, population):\n",
    "        \"\"\"Gives a list of all possible paths in the network to the environment. \n",
    "           The environment could then assign an action number to each path. \n",
    "        \"\"\"\n",
    "        if population == 0:\n",
    "            return (\"01\", \"051\", \"0451\")\n",
    "        elif population == 1:\n",
    "            return (\"23\", \"243\", \"2453\")\n",
    "        else:\n",
    "            return \"no such population\"\n",
    "        \n",
    "    def shared_link(self): # a simple link for this example, need more generalized utility function for more comlicated networks\n",
    "        return \"45\" \n",
    "    \n",
    "    def calculate_ttime(self, flows): # flows now is a dictionary; add flow before feeding into the cost fct\n",
    "        \"\"\"Given a dictionary of paths and flows, this function returns a dictionary of \n",
    "           paths and travel time (secs), a.k.a ttime.\n",
    "           \n",
    "           Returns: \n",
    "               travel_times (dictionary): A dictionary of paths to their travel times\n",
    "        \"\"\"\n",
    "        congestion = {}\n",
    "        for population in flows:\n",
    "            for path in flows[str(population)]:\n",
    "                links = self.__paths[path]\n",
    "                for link in links:\n",
    "                    if link not in self.__links:\n",
    "                        break\n",
    "                    if link not in congestion:\n",
    "                        congestion[link] = 0\n",
    "                    congestion[link] += flows[str(population)][path]\n",
    "                    \n",
    "        t_time = {}\n",
    "        for population in flows:\n",
    "            t_time[population] = {}\n",
    "            for path in flows[population]:\n",
    "                total_time = 0\n",
    "                # Calculate travel time of path by adding the congestion time of every \n",
    "                # link in that path\n",
    "                links = self.__paths[path]\n",
    "                for link in links:\n",
    "                    if link not in self.__links:\n",
    "                        break\n",
    "                    t_time_func = self.__links[link]\n",
    "                    total_time += t_time_func(congestion[link])\n",
    "                t_time[population][path] = total_time\n",
    "        \n",
    "        return t_time\n",
    "        \n",
    "        \n",
    "    def calculate_ttime_lambda(self, flows, Lambda):\n",
    "        \"\"\"Given a dictionary of paths and flows, this function returns a dictionary of \n",
    "           paths and travel time, considering the social factor lambda (secs).\n",
    "           \n",
    "           Returns: \n",
    "               travel_times (dictionary): A dictionary of paths to their travel times,\n",
    "               considering the social factor lambda\n",
    "        \"\"\"\n",
    "        congestion = {}\n",
    "        for population in flows:\n",
    "            for path in flows[str(population)]:\n",
    "                links = self.__paths[path]\n",
    "                for link in links:\n",
    "                    if link not in self.__links:\n",
    "                        break\n",
    "                    if link not in congestion:\n",
    "                        congestion[link] = 0\n",
    "                    congestion[link] += flows[str(population)][path]\n",
    "        \n",
    "        t_time_lambda  = {}\n",
    "        for population in flows:\n",
    "            t_time_lambda[population] = {}\n",
    "            for path in flows[population]:\n",
    "                total_time = 0\n",
    "                # Calculate travel time of path by adding the congestion time of every \n",
    "                # link in that path\n",
    "                links = self.__paths[path]\n",
    "                for link in links:\n",
    "                    if link not in self.__links:\n",
    "                        break\n",
    "                    if link == \"01\" or link == \"05\" or link == \"43\" or link == \"23\":\n",
    "                        total_time += Lambda * congestion[link]\n",
    "                    elif link == \"04\":\n",
    "                        total_time += Lambda * congestion[link] * 0.5\n",
    "                    elif link == \"51\":\n",
    "                        total_time += Lambda * congestion[link] / 3\n",
    "                    elif link == \"45\":\n",
    "                        total_time += Lambda * congestion[link] * 3\n",
    "                    elif link == \"53\":\n",
    "                        total_time += Lambda * congestion[link] / 4\n",
    "                    t_time_func = self.__links[link]\n",
    "                    total_time += t_time_func(congestion[link])\n",
    "                t_time_lambda [population][path] = total_time\n",
    "        return t_time_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the environments\n",
    "# All environments in this project\n",
    "routing_envs = {\"Braess\": BraessEnv}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Configurations\n",
    "\n",
    "EXP_NUM = 0\n",
    "# time horizon of a single rollout\n",
    "HORIZON = 1\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 1\n",
    "# number of parallel workers\n",
    "N_CPUS = 2\n",
    "# number of steps\n",
    "T = 10000\n",
    "\n",
    "\n",
    "single_env = \"braess\"\n",
    "    \n",
    "    \n",
    "    \n",
    "def register_env(env_name):\n",
    "    try:\n",
    "        register(\n",
    "            id=env_name,\n",
    "            entry_point=\"BraessEnv\", # Check if this is correct.\n",
    "            kwargs={\n",
    "                \"env_params\": {},\n",
    "                \"network\": {},\n",
    "            })\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def run_model(rollout_size=1, num_steps=T):\n",
    "    \"\"\"Run the model for num_steps if provided. The total rollout length is rollout_size.\"\"\"\n",
    "    register_env(single_env)\n",
    "    env = DummyVecEnv([lambda: gym.envs.make(single_env)])  # The algorithms require a vectorized environment to run\n",
    "\n",
    "    model = PPO2('MlpPolicy', env, verbose=1, n_steps=rollout_size)\n",
    "    model.learn(total_timesteps=num_steps)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestRunModel(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        pass\n",
    "    \n",
    "    def testEnvsCanBeLoadedViaModules(self):\n",
    "        \n",
    "    \n",
    "    \n",
    "    def testModelIsWhatIExpectItToBe:\n",
    "        \"\"\"This code tests whether the model made from 'run_model' is set up for the iterative game.\n",
    "        This means:\n",
    "         - n_steps = 1 # This is the number of environment steps per update\n",
    "         - gamma = 0.99 - or - gamma_i = 20/(10 + i)\n",
    "         - nminibatches: # (int) Number of training minibatches per update\n",
    "         - noptepochs: # (int) Number of epoch when optimizing the surrogate\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def testEnvConstructorCreatesHowIExpectTheEnvToBeMade:\n",
    "        pass\n",
    "    \n",
    "    def testAnEnvIsSuccessfullyRegisteredInGym:\n",
    "        pass\n",
    "    \n",
    "    def testAnyOtherFunctionalityThatsSpecificToStableBaselines:\n",
    "        pass\n",
    "    \n",
    "    def testPipelineWork(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NUM = 0\n",
    "# time horizon of a single rollout\n",
    "HORIZON = 1\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 1\n",
    "# number of parallel workers\n",
    "N_CPUS = 1\n",
    "# number of steps\n",
    "T = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Add this as the last thing to do for stable baseline conversion.\n",
    "\n",
    "    model = run_model(N_ROLLOUTS, T)\n",
    "    # Save the model to a desired folder and then delete it to demonstrate loading\n",
    "#     if not os.path.exists(os.path.realpath(os.path.expanduser('~/baseline_results'))):\n",
    "#         os.makedirs(os.path.realpath(os.path.expanduser('~/baseline_results')))\n",
    "#     path = os.path.realpath(os.path.expanduser('~/baseline_results'))\n",
    "#     save_path = os.path.join(path, args.result_name)\n",
    "\n",
    "    self.file = open(filename2, \"w+\")\n",
    "    self.file.close()\n",
    "    print('Saving the trained model!')\n",
    "    model.save(filename2)\n",
    "    del model\n",
    "\n",
    "    # Replay the result by loading the model\n",
    "    print('Loading the trained model and testing it out!')\n",
    "    model = PPO2.load(filename2)\n",
    "#     flow_params = get_flow_params(os.path.join(path, args.result_name) + '.json')\n",
    "#     flow_params['sim'].render = True\n",
    "    env = DummyVecEnv([lambda: BraessEnv()])  # The algorithms require a vectorized environment to run\n",
    "    obs = env.reset()\n",
    "    reward = 0\n",
    "    # Should I have it run only once or multiple times to test it out. I'll probably want to run it multiple times\n",
    "    # Currently HORIZON is only 1\n",
    "    for i in range(HORIZON):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        reward += rewards\n",
    "    print('the final reward is {}'.format(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# MDP: (All characteristics of this MDP is given in W. Krichene's Paper \n",
    "#                            -- \"Learning Nash Equilibria in Congestion Games\")\n",
    "#\n",
    "#  - Observations/States: Each player will observe the cost (travel time) on all of the paths (according to paper)\n",
    "#                         If the player only observes the loss she incurs then it becomes a multiarmed bandit \n",
    "#                         setting.\n",
    "#  - Actions: Each player will choose a path, using a randomized/mixed strategy. \n",
    "#             This means we have a stochastic policy.\n",
    "#  - Reward: The *cost* of each player will be the travel time that they've incurred on their path. T\n",
    "#            Each player wants to minimize their travel time. For reward, we can maximize the negative cost.\n",
    "#  - Model of the environment: We don't have one in this case\n",
    "#\n",
    "###\n",
    "\n",
    "# Test 1: Test that reset() returns an initial observation. \n",
    "#         The initial observation should be:\n",
    "#                \"ABD\": 3\n",
    "#                \"ACD\": 3\n",
    "#                \"ABCD\": 2.25\n",
    "class TestBraessEnv(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        configs = {}\n",
    "        self.env = BraessEnv(configs)\n",
    "#         Representing 1 flow of .01 taking path \"ABD\"\n",
    "#         flow = {\"ABD\": 0.01,\n",
    "#                 \"ACD\": 0,\n",
    "#                 \"ABCD\": 0}\n",
    "        self.action = {\"population_1\": np.array([0.01, 0, 0])}  \n",
    "    \n",
    "#         flows = {\n",
    "#             \"ABD\": 0,\n",
    "#             \"ACD\": 0,\n",
    "#             \"ABCD\": 0.01\n",
    "#         } # Flow of 0.01 taking \"ABCD\"\n",
    "        self.action2 = {\"population_1\": np.array([0, 0, 0.01])}\n",
    "        \n",
    "    def testFormatOfObservationsandRewardsInStepandReset(self):\n",
    "        initial = self.env.reset()\n",
    "        items = list(initial.items())\n",
    "        self.assertEqual(len(items), 1)\n",
    "        \n",
    "        agent, obs = items[0]\n",
    "        \n",
    "        # Seems like the \"preprocessor\" in the reset function turns the dictionary into an numpy array\n",
    "        # Just check if it has 3 values associated with 3 paths\n",
    "        self.assertEqual(len(obs), 3)\n",
    "        \n",
    "        next_obs, reward, done, info = self.env.step(self.action)\n",
    "        \n",
    "        obs_items = list(next_obs.items())\n",
    "        reward_items = list(reward.items())\n",
    "        self.assertEqual(len(obs_items), 1)\n",
    "        self.assertEqual(len(reward_items), 1)\n",
    "        \n",
    "        actual_obs = obs_items[0][1]\n",
    "        actual_reward = reward_items[0][1]\n",
    "        \n",
    "        self.assertEqual(len(actual_obs), 3)\n",
    "        self.assertTrue(isinstance(actual_reward, float))\n",
    "        self.assertTrue(done[\"__all__\"])\n",
    "        \n",
    "        \n",
    "    def testResetReturnsObservation(self):\n",
    "        initial = self.env.reset()       \n",
    "        obs = list(initial.values())[0]\n",
    "        \n",
    "#         dict_expected = {\n",
    "#             \"ABD\": 3,\n",
    "#             \"ACD\": 3,\n",
    "#             \"ABCD\": 2.25\n",
    "#         }\n",
    "        actual_expected = np.array([3, 3, 2.25])\n",
    "        np.testing.assert_array_equal(obs, actual_expected)\n",
    "        \n",
    "    def testStepReturnsCorrectInformation(self):\n",
    "        # Test that step returns correct next observations, reward, and termination signal\n",
    "        env = self.env\n",
    "        env.reset()\n",
    "\n",
    "        next_obs, reward, done, _ = env.step(self.action)\n",
    "        # dict_obs = {\n",
    "        #     \"ABD\": 3.01,\n",
    "        #     \"ACD\": 3,\n",
    "        #     \"ABCD\": 2.26\n",
    "        # }\n",
    "        actual_expected_obs = [3.01, 3, 2.26]\n",
    "        \n",
    "        # travel_times = {\n",
    "        #     \"ABD\": 3.01,\n",
    "        #     \"ACD\": 3,\n",
    "        #     \"ABCD\": 2.26\n",
    "        # }\n",
    "        flow_dist = np.array([0.01, 0, 0])\n",
    "        path_rewards = np.array([-3.01, -3, -2.26]) # Rewards are negative of travel times\n",
    "        expected_reward = np.dot(flow_dist, path_rewards)\n",
    "        \n",
    "        np.testing.assert_array_equal(actual_expected_obs, list(next_obs.values())[0])\n",
    "        self.assertEqual(expected_reward, list(reward.values())[0])\n",
    "        self.assertTrue(done[\"__all__\"])\n",
    "        \n",
    "    def testStepSavesNoPrevInfo(self):\n",
    "        env = self.env\n",
    "        env.reset()\n",
    "        env.step(self.action)\n",
    "    \n",
    "        next_obs, reward, done, _ = env.step(self.action2)\n",
    "        \n",
    "#         dict_obs = {\n",
    "#             \"ABD\": 3.01,\n",
    "#             \"ACD\": 3.01,\n",
    "#             \"ABCD\": 2.27\n",
    "#         }\n",
    "#         travel_times = {\n",
    "#             \"ABD\": 3.01,\n",
    "#             \"ACD\": 3.01,\n",
    "#             \"ABCD\": 2.27\n",
    "#         }\n",
    "        \n",
    "        expected_obs = np.array([3.01, 3.01, 2.27])\n",
    "        \n",
    "        flow_dist = np.array([0, 0, 0.01])\n",
    "        path_rewards = np.array([-3.01, -3.01, -2.27])\n",
    "        expected_reward = np.dot(flow_dist, path_rewards)\n",
    "        \n",
    "        np.testing.assert_array_equal(expected_obs, list(next_obs.values())[0])\n",
    "        self.assertEqual(expected_reward, list(reward.values())[0])\n",
    "        self.assertTrue(done[\"__all__\"])\n",
    "        \n",
    "    def testEnvDoesNotKeepActionsFromPreviousRun(self):\n",
    "#         env = self.env\n",
    "#         # Episode 1\n",
    "#         env.reset()\n",
    "#         env.step(self.action)\n",
    "        \n",
    "#         # Episode 2\n",
    "#         env.reset()\n",
    "# #         flows = {\n",
    "# #             \"ABD\": 0,\n",
    "# #             \"ACD\": 0,\n",
    "# #             \"ABCD\": 0.01\n",
    "# #         } # Flow of 0.01 taking \"ABCD\"\n",
    "# # .     {\"population_1\": np.array([0, 0, 0.01])}\n",
    "    \n",
    "#         env.step(self.action2)\n",
    "        \n",
    "#         # Check\n",
    "#         self.fail()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = BraessNetwork()\n",
    "{route: 0 for route in n.routes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "network = BraessNetwork()\n",
    "\n",
    "# Test 1 for calculate ttime\n",
    "#\n",
    "#                  B\n",
    "#                / | \\                   \n",
    "#             /    |    \\\n",
    "#          A       |       D\n",
    "#             \\    |    /\n",
    "#                \\ | /\n",
    "#                  C\n",
    "#\n",
    "# Out of 100 cars, we will do: \n",
    "#     ABD = 25; \n",
    "#     ACD = 25; \n",
    "#     ABCD = 50 \n",
    "# \n",
    "# The travel time on each path should result as:\n",
    "#     ABD = 3.75 (units)\n",
    "#     ACD = 3.75 (units)\n",
    "#     ABCD = 3.75 (units)\n",
    "#\n",
    "flows = {\n",
    "    \"ABD\": 0.25,\n",
    "    \"ACD\": 0.25,\n",
    "    \"ABCD\": 0.50\n",
    "}\n",
    "expect1 = {\n",
    "    \"ABD\": 3.75,\n",
    "    \"ACD\": 3.75,\n",
    "    \"ABCD\": 3.75\n",
    "}\n",
    "times = network.calculate_ttime(flows)\n",
    "print(\"This is what was given: \" + str(times))\n",
    "print(\"This is what I expect: \" + str(expect1))\n",
    "print(\"---\")\n",
    "\n",
    "# Test 2 for calculate ttime\n",
    "# Out of 100 cars, we will do: \n",
    "#     ABD = 50; \n",
    "#     ACD = 50; \n",
    "#     ABCD = 0 \n",
    "# \n",
    "# The travel time on each path should result as:\n",
    "#     ABD = 3.5 (units)\n",
    "#     ACD = 3.5 (units)\n",
    "#     ABCD = 3.25 (units)  - Even though no one's using this path\n",
    "flows = {\n",
    "    \"ABD\": 0.50,\n",
    "    \"ACD\": 0.50,\n",
    "    \"ABCD\": 0\n",
    "}\n",
    "expect2 = {\n",
    "    \"ABD\": 3.5,\n",
    "    \"ACD\": 3.5,\n",
    "    \"ABCD\": 3.25\n",
    "}\n",
    "times = network.calculate_ttime(flows)\n",
    "print(\"This is what was given: \" + str(times))\n",
    "print(\"This is what I expect: \" + str(expect2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to run experiments\n",
    "gamma = 0.5\n",
    "single_pop_network = BraessEnv({})\n",
    "config = {\"gamma\": gamma}\n",
    "policy_graphs = {\n",
    "    'population': (PPOPolicyGraph, single_pop_network.observation_space, single_pop_network.action_space, config)\n",
    "}\n",
    "\n",
    "BRAESS_CONFIG = {\n",
    "    # === Environment ===\n",
    "    # Discount factor of the MDP\n",
    "    \"gamma\": gamma, # Ask Yiling\n",
    "    # Number of steps after which the episode is forced to terminate. Defaults\n",
    "    # to `env.spec.max_episode_steps` (if present) for Gym envs.\n",
    "    \"horizon\": 1,\n",
    "    # Calculate rewards but don't reset the environment when the horizon is\n",
    "    # hit. This allows value estimation and RNN state to span across logical\n",
    "    # episodes denoted by horizon. This only has an effect if horizon != inf.\n",
    "    \"soft_horizon\": True,\n",
    "    # Don't set 'done' at the end of the episode. Note that you still need to\n",
    "    # set this if soft_horizon=True, unless your env is actually running\n",
    "    # forever without returning done=True.\n",
    "    \"no_done_at_end\": True,\n",
    "    # The default learning rate\n",
    "    \"lr\": 0.0001, # Ask Yiling\n",
    "\n",
    "    # === Evaluation ===\n",
    "    # Evaluate with every `evaluation_interval` training iterations.\n",
    "    # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "    # Note that evaluation is currently not parallelized, and that for Ape-X\n",
    "    # metrics are already only reported for the lowest epsilon workers.\n",
    "    \"evaluation_interval\": None,\n",
    "    # Number of episodes to run per evaluation period.\n",
    "    \"evaluation_num_episodes\": 1,\n",
    "\n",
    "    # === Multiagent ===\n",
    "    \"multiagent\": {\n",
    "        'policy_graphs': policy_graphs,\n",
    "        # Function mapping agent ids to policy ids.\n",
    "        \"policy_mapping_fn\": tune.function(lambda agent_id: 'population')\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env_creator_name = 'multi_routing'\n",
    "    register_env(env_creator_name, lambda config: BraessEnv(config))\n",
    "    ray.init()\n",
    "    experiments = {\n",
    "        'route-DQN': {\n",
    "            'run': 'PPO',\n",
    "            'env': 'multi_routing',\n",
    "            'stop': {\n",
    "                'training_iteration': 5\n",
    "            },\n",
    "            'config': BRAESS_CONFIG\n",
    "        },\n",
    "        # put additional experiments to run concurrently here\n",
    "    }\n",
    "    \n",
    "    run_experiments(experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots \n",
    "\n",
    "The following code plots the RL results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "file = open(filename, 'r')\n",
    "j = 0\n",
    "# we want to plot the evolution of the flow distribution, of the travel time, and of the reward/average travel time\n",
    "Actions_plot = np.array([[0, 0, 0]]) # flow path\n",
    "Reward_plot = np.array([0]) # average travel time/cost or reward\n",
    "Travel_time_plot = np.array([[0, 0, 0]]) # travel time on each path\n",
    "while(True):\n",
    "    j = j+1\n",
    "    data = file.readline()\n",
    "#     if j%100 != 1:\n",
    "#         continue\n",
    "    try:\n",
    "#         action_dict = ast.literal_eval(\"{\" + actions.split('{')[1].split('}')[0]+ \"}\")\n",
    "#         reward_dict = ast.literal_eval(\"{\" + rewards.split('{')[1].split('}')[0]+ \"}\")\n",
    "        \n",
    "        data = data.split(';')\n",
    "        \n",
    "    \n",
    "#         network = Networks.network(network_name, nb_veh)\n",
    "#         travel_time, marginal_cost, rew_dict = get_tt_mc(action_dict, network, 0)\n",
    "        \n",
    "#         actions_np = np.fromiter(action_dict.values(), dtype=int)\n",
    "#         Actions_plot = np.append(Actions_plot, [actions_np], axis=0)\n",
    "#         rewards_np = np.fromiter(reward_dict.values(), dtype=float)\n",
    "#         Reward_plot = np.append(Reward_plot, [rewards_np], axis=0)\n",
    "#         travel_time_np = np.fromiter(travel_time.values(), dtype=float)\n",
    "#         Travel_time_plot = np.append(Travel_time_plot, [travel_time_np], axis=0)\n",
    "#         if(j==1):\n",
    "#             print(\"------ First iteration ------\")\n",
    "#             print(\"Path choice: \" + str(action_dict))\n",
    "#             print(\"Reward ray: \" + str(reward_dict))\n",
    "#             print(\"Travel time paths: \" + str({\"path \" + str(i): network.travel_time(i) for i in range(3)}))\n",
    "#             print(\"Travel time cars: \" + str(travel_time))\n",
    "#             print(\"Marginal cost: \" + str(marginal_cost))\n",
    "#             print(\"Reward network: \" + str(rew_dict))\n",
    "    except:\n",
    "        print()\n",
    "        print(\"------ Last iteration ------\")\n",
    "        print(\"Path choice: \" + str(action_dict))\n",
    "        print(\"Reward ray: \" + str(reward_dict))\n",
    "        print(\"Travel time paths: \" + str({\"path \" + str(i): network.travel_time(i) for i in range(3)}))\n",
    "        print(\"Travel time cars: \" + str(travel_time))\n",
    "        print(\"Marginal cost: \" + str(marginal_cost))\n",
    "        print(\"Reward network: \" + str(rew_dict))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
