{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/marl_routing/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from gym import Env, spaces\n",
    "import unittest\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import collections\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "from gym.envs.registration import EnvSpec, register\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.dqn.dqn_policy_graph import *\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import *\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "from ray.rllib.models.preprocessors import DictFlatteningPreprocessor, Preprocessor\n",
    "\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/Users/mtgibson/learning_wardrop/graph_results.txt\"\n",
    "\n",
    "filename2 = \"/Users/mtgibson/learning_wardrop/model_results.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BraessEnv(MultiAgentEnv):\n",
    "    \"\"\"Traffic Environment that uses the Braess's network. \n",
    "       See https://github.com/openai/gym/blob/master/gym/core.py for more details.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params=None):\n",
    "        # Make the Braess's network\n",
    "        self.network = BraessNetwork()\n",
    "        self.routes = self.network.routes\n",
    "        \n",
    "        # Observation space contains each route and travel times\n",
    "        self.num_routes = len(self.routes)\n",
    "        self.observation_space = spaces.Box(low=0,high=float('+inf'),shape=(3,),dtype=np.float32)\n",
    "        \n",
    "        # Action space contains each route and flow distribution of population (decimal)\n",
    "        self.action_space = action_spaces = spaces.Box(low=0,high=1,shape=(3,),dtype=np.float32)\n",
    "        \n",
    "        self.reward_range = (-float('inf'), 0)\n",
    "        \n",
    "        #Storage bins for data\n",
    "        self.__path_flows = []\n",
    "        self.__travel_times = []\n",
    "        self.__avg_times = []\n",
    "        return\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        \"\"\"Run one timestep of the environment's dynamics. \n",
    "        \n",
    "        Env Step Procedure:\n",
    "            (1) takes in routing distribution - comes from the action, \n",
    "            (2) calculate travel times for each path given flow on each path, \n",
    "            (3) return the reward which is the negative of the travel time.\n",
    "            \n",
    "        Note: We will give a reward to each path and will return an array/dictionary \n",
    "              as the reward for the population.\n",
    "            \n",
    "        Args:\n",
    "            action (dictionary): A dictionary where the key is a population and values are another dictionary where\n",
    "                                 the key is a path and the value is a flow distribution.\n",
    "                                 We assume there is only 1 agent and thus one o-d pair\n",
    "        \n",
    "        Returns:\n",
    "            next_observation (array): the travel times determined for each path\n",
    "            reward (float): -1*travel_time_of_agent\n",
    "            done (boolean): _\n",
    "            info (dict): other information needed - don't really need now though\n",
    "        \"\"\"\n",
    "        obs_dict, rew_dict, done, info_dict = {}, {}, {}, {}\n",
    "        \n",
    "        \n",
    "        for agent, action in action_dict.items():\n",
    "            # Create action dictionary\n",
    "            action_dict = {}\n",
    "            for i in range(len(action)):\n",
    "                action_dict[self.routes[i]] = action[i]\n",
    "            \n",
    "            # Calculate the travel times and store flow distributions and travel times (Edit for Multi-agent)\n",
    "            travel_times_dict = self.network.calculate_ttime(action_dict)\n",
    "            self.__travel_times.append(travel_times_dict)\n",
    "            \n",
    "            #Transform dictionary into list\n",
    "            travel_times = []\n",
    "            for route in self.routes:\n",
    "                travel_times.append(travel_times_dict[route])\n",
    "            \n",
    "            # Calculate the reward for the population - mean (negative) travel time (Edit for MA w/ different routes)\n",
    "            reward = np.dot(np.array(action), \n",
    "                            -1*np.array(travel_times))\n",
    "            \n",
    "            obs_dict[agent] = np.array(travel_times) \n",
    "            rew_dict[agent] = reward\n",
    "            done[agent] = True\n",
    "            info_dict[agent] = {}\n",
    "            \n",
    "            # Add data to the storage bins\n",
    "            self.add_data(action, travel_times, reward)\n",
    "                            \n",
    "        done[\"__all__\"] = True\n",
    "        return obs_dict, rew_dict, done, info_dict\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the state of the environment and returns an initial observation.\n",
    "        \n",
    "        For the initial observation: Make an array of 3 elements corresponding to 3 paths in\n",
    "        the Braess network. It should have the format ---\n",
    "        \n",
    "        state = [<traveltime_ABD>, <traveltime_ACD>, <traveltime_ABCD>] = [2, 2, 0.25]\n",
    "        \"\"\"\n",
    "        # Calculate initial travel times with 0 flow on the network\n",
    "        flows = {route: 0 for route in self.routes}\n",
    "        initial_dict = self.network.calculate_ttime(flows)\n",
    "        \n",
    "        # Turn initial observation to an array\n",
    "        t_0 = []\n",
    "        for route in self.routes:\n",
    "            t_0.append(initial_dict[route])\n",
    "        \n",
    "        # State will be a numpy array\n",
    "        self.state = {'population_1': np.array(t_0)}\n",
    "        \n",
    "        # Reset the data bins and insert first item\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "        self.file = open(filename, \"a+\")\n",
    "        self.file.write(str(t_0) + \"\\n\")\n",
    "        self.file.close()\n",
    "        \n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    def add_data(self, path_flow, travel_time, reward):\n",
    "        self.file = open(filename, \"a\")\n",
    "        self.file.write(str(path_flow) + ';' + str(travel_time) + ';' + str(reward) + '\\n')\n",
    "        self.file.close()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BraessNetwork(object):\n",
    "    \"\"\"Stores the cost for all links. Handles calculating the cost of a path given action\n",
    "       of every car.\n",
    "    \"\"\"\n",
    "    def __init__(self, params=None):\n",
    "        self.__links = {\n",
    "            \"AB\": lambda f: 1 + (f/100),\n",
    "            \"AC\": lambda _: 2,\n",
    "            \"BD\": lambda _: 2,\n",
    "            \"CD\": lambda f: 1 + (f/100),\n",
    "            \"BC\": lambda _: 0.25\n",
    "        } # Dictionary of links and their congestion functions\n",
    "        self.__paths = {\n",
    "            \"ABD\": (\"AB\", \"BD\"),\n",
    "            \"ACD\": (\"AC\", \"CD\"),\n",
    "            \"ABCD\": (\"AB\", \"BC\", \"CD\")\n",
    "        } # Dictionaries of paths to links\n",
    "        self.total_flow = 100  # 100 cars in total on this network\n",
    "        return \n",
    "    \n",
    "    @property\n",
    "    def routes(self):\n",
    "        \"\"\"Gives a list of all possible paths in the network to the environment. \n",
    "           The environment could then assign an action number to each path. \n",
    "        \"\"\"\n",
    "        return (\"ABD\", \"ACD\", \"ABCD\")\n",
    "    \n",
    "    def calculate_ttime(self, flows):\n",
    "        \"\"\"Given a dictionary of paths and flows, this function returns a dictionary of \n",
    "           paths and travel time (secs), a.k.a ttime.\n",
    "           \n",
    "           Arg:\n",
    "               flows (dictionary): A dictionare where the key correspond to a path in the network of one o-d pair\n",
    "                                   and the value corresponds to the flow on that path. Flow will be a float\n",
    "                                   between 0 and 1 represent the percent of flow. \n",
    "           \n",
    "           Returns: \n",
    "               travel_times (list): A list of travel times, order matters \n",
    "                                    --> according to the order in my list of paths.\n",
    "        \"\"\"\n",
    "        congestion = {}\n",
    "        for path in flows:\n",
    "            links = self.__paths[path]\n",
    "            for link in links:\n",
    "                if link not in congestion:\n",
    "                    congestion[link] = 0\n",
    "                congestion[link] += flows[path] * self.total_flow\n",
    "        \n",
    "        t_time = {}\n",
    "        for path in flows:\n",
    "            total_time = 0\n",
    "            # Calculate travel time of path by adding the congestion time of every \n",
    "            # link in that path\n",
    "            links = self.__paths[path]\n",
    "            for link in links:\n",
    "                t_time_func = self.__links[link]\n",
    "                total_time += t_time_func(congestion[link])\n",
    "            t_time[path] = total_time\n",
    "        \n",
    "        \n",
    "        return t_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network2(object):\n",
    "    \"\"\"Stores the cost for all links. Handles calculating the cost of a path given action\n",
    "       of every car.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.__links = {\n",
    "            \"01\": lambda f: f + 2., \n",
    "            \"04\": lambda f: f/2,\n",
    "            \"05\": lambda f: f,\n",
    "            \"51\": lambda f: f/3,\n",
    "            \"45\": lambda f: 3*f, \n",
    "            \"43\": lambda f: f, \n",
    "            \"24\": lambda _: 0.5,\n",
    "            \"23\": lambda f: f + 1.,\n",
    "            \"53\": lambda f: f/4\n",
    "        } # Dictionary of links and their congestion functions\n",
    "        self.__paths = {\n",
    "            \"01\": (\"01\",\"11\"),\n",
    "            \"051\": (\"05\", \"51\"),\n",
    "            \"0451\": (\"04\", \"45\", \"51\"),\n",
    "            \"23\": (\"23\",\"33\"),\n",
    "            \"243\": (\"24\",\"43\"),\n",
    "            \"2453\": (\"24\",\"45\",\"53\")\n",
    "        } # Dictionaries of paths to links\n",
    "        return \n",
    "    \n",
    "    def paths(self, population):\n",
    "        \"\"\"Gives a list of all possible paths in the network to the environment. \n",
    "           The environment could then assign an action number to each path. \n",
    "        \"\"\"\n",
    "        if population == 0:\n",
    "            return (\"01\", \"051\", \"0451\")\n",
    "        elif population == 1:\n",
    "            return (\"23\", \"243\", \"2453\")\n",
    "        else:\n",
    "            return \"no such population\"\n",
    "        \n",
    "    def shared_link(self): # a simple link for this example, need more generalized utility function for more comlicated networks\n",
    "        return \"45\" \n",
    "    \n",
    "    def calculate_ttime(self, flows): # flows now is a dictionary; add flow before feeding into the cost fct\n",
    "        \"\"\"Given a dictionary of paths and flows, this function returns a dictionary of \n",
    "           paths and travel time (secs), a.k.a ttime.\n",
    "           \n",
    "           Returns: \n",
    "               travel_times (dictionary): A dictionary of paths to their travel times\n",
    "        \"\"\"\n",
    "        congestion = {}\n",
    "        for population in flows:\n",
    "            for path in flows[str(population)]:\n",
    "                links = self.__paths[path]\n",
    "                for link in links:\n",
    "                    if link not in self.__links:\n",
    "                        break\n",
    "                    if link not in congestion:\n",
    "                        congestion[link] = 0\n",
    "                    congestion[link] += flows[str(population)][path]\n",
    "                    \n",
    "        t_time = {}\n",
    "        for population in flows:\n",
    "            t_time[population] = {}\n",
    "            for path in flows[population]:\n",
    "                total_time = 0\n",
    "                # Calculate travel time of path by adding the congestion time of every \n",
    "                # link in that path\n",
    "                links = self.__paths[path]\n",
    "                for link in links:\n",
    "                    if link not in self.__links:\n",
    "                        break\n",
    "                    t_time_func = self.__links[link]\n",
    "                    total_time += t_time_func(congestion[link])\n",
    "                t_time[population][path] = total_time\n",
    "        \n",
    "        return t_time\n",
    "        \n",
    "        \n",
    "    def calculate_ttime_lambda(self, flows, Lambda):\n",
    "        \"\"\"Given a dictionary of paths and flows, this function returns a dictionary of \n",
    "           paths and travel time, considering the social factor lambda (secs).\n",
    "           \n",
    "           Returns: \n",
    "               travel_times (dictionary): A dictionary of paths to their travel times,\n",
    "               considering the social factor lambda\n",
    "        \"\"\"\n",
    "        congestion = {}\n",
    "        for population in flows:\n",
    "            for path in flows[str(population)]:\n",
    "                links = self.__paths[path]\n",
    "                for link in links:\n",
    "                    if link not in self.__links:\n",
    "                        break\n",
    "                    if link not in congestion:\n",
    "                        congestion[link] = 0\n",
    "                    congestion[link] += flows[str(population)][path]\n",
    "        \n",
    "        t_time_lambda  = {}\n",
    "        for population in flows:\n",
    "            t_time_lambda[population] = {}\n",
    "            for path in flows[population]:\n",
    "                total_time = 0\n",
    "                # Calculate travel time of path by adding the congestion time of every \n",
    "                # link in that path\n",
    "                links = self.__paths[path]\n",
    "                for link in links:\n",
    "                    if link not in self.__links:\n",
    "                        break\n",
    "                    if link == \"01\" or link == \"05\" or link == \"43\" or link == \"23\":\n",
    "                        total_time += Lambda * congestion[link]\n",
    "                    elif link == \"04\":\n",
    "                        total_time += Lambda * congestion[link] * 0.5\n",
    "                    elif link == \"51\":\n",
    "                        total_time += Lambda * congestion[link] / 3\n",
    "                    elif link == \"45\":\n",
    "                        total_time += Lambda * congestion[link] * 3\n",
    "                    elif link == \"53\":\n",
    "                        total_time += Lambda * congestion[link] / 4\n",
    "                    t_time_func = self.__links[link]\n",
    "                    total_time += t_time_func(congestion[link])\n",
    "                t_time_lambda [population][path] = total_time\n",
    "        return t_time_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the environments\n",
    "# All environments in this project\n",
    "routing_envs = {\"Braess\": BraessEnv}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Configurations\n",
    "\n",
    "EXP_NUM = 0\n",
    "# time horizon of a single rollout\n",
    "HORIZON = 1\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 1\n",
    "# number of parallel workers\n",
    "N_CPUS = 2\n",
    "# number of steps\n",
    "T = 10000\n",
    "\n",
    "\n",
    "single_env = \"braess\"\n",
    "    \n",
    "    \n",
    "    \n",
    "def register_env(env_name):\n",
    "    try:\n",
    "        register(\n",
    "            id=env_name,\n",
    "            entry_point=\"BraessEnv\", # Check if this is correct.\n",
    "            kwargs={\n",
    "                \"env_params\": {},\n",
    "                \"network\": {},\n",
    "            })\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def run_model(rollout_size=1, num_steps=T):\n",
    "    \"\"\"Run the model for num_steps if provided. The total rollout length is rollout_size.\"\"\"\n",
    "    register_env(single_env)\n",
    "    env = DummyVecEnv([lambda: gym.envs.make(single_env)])  # The algorithms require a vectorized environment to run\n",
    "\n",
    "    model = PPO2('MlpPolicy', env, verbose=1, n_steps=rollout_size)\n",
    "    model.learn(total_timesteps=num_steps)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-33-591afd95e99d>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-591afd95e99d>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    self.\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class TestRunModel(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        pass\n",
    "    \n",
    "    def testEnvsCanBeLoadedViaModules(self):\n",
    "        \n",
    "    \n",
    "    \n",
    "    def testModelIsWhatIExpectItToBe:\n",
    "        \"\"\"This code tests whether the model made from 'run_model' is set up for the iterative game.\n",
    "        This means:\n",
    "         - n_steps = 1 # This is the number of environment steps per update\n",
    "         - gamma = 0.99 - or - gamma_i = 20/(10 + i)\n",
    "         - nminibatches: # (int) Number of training minibatches per update\n",
    "         - noptepochs: # (int) Number of epoch when optimizing the surrogate\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def testEnvConstructorCreatesHowIExpectTheEnvToBeMade:\n",
    "        pass\n",
    "    \n",
    "    def testAnEnvIsSuccessfullyRegisteredInGym:\n",
    "        pass\n",
    "    \n",
    "    def testAnyOtherFunctionalityThatsSpecificToStableBaselines:\n",
    "        pass\n",
    "    \n",
    "    def testPipelineWork(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NUM = 0\n",
    "# time horizon of a single rollout\n",
    "HORIZON = 1\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 1\n",
    "# number of parallel workers\n",
    "N_CPUS = 1\n",
    "# number of steps\n",
    "T = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Attempted to look up malformed environment ID: b'braess'. (Currently all IDs must be of the form ^(?:[\\w:-]+\\/)?([\\w:.-]+)-v(\\d+)$.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a1ca8c6a4fc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Add this as the last thing to do for stable baseline conversion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_ROLLOUTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Save the model to a desired folder and then delete it to demonstrate loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     if not os.path.exists(os.path.realpath(os.path.expanduser('~/baseline_results'))):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-01e7ee2db6d0>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(rollout_size, num_steps)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m\"\"\"Run the model for num_steps if provided. The total rollout length is rollout_size.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mregister_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# The algorithms require a vectorized environment to run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MlpPolicy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrollout_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/marl_routing/lib/python3.5/site-packages/stable_baselines/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_fns)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_fns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mVecEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/marl_routing/lib/python3.5/site-packages/stable_baselines/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_fns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mVecEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-01e7ee2db6d0>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m\"\"\"Run the model for num_steps if provided. The total rollout length is rollout_size.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mregister_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# The algorithms require a vectorized environment to run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MlpPolicy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrollout_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/marl_routing/lib/python3.5/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/marl_routing/lib/python3.5/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/marl_routing/lib/python3.5/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_id_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to look up malformed environment ID: {}. (Currently all IDs must be of the form {}.)'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_id_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Attempted to look up malformed environment ID: b'braess'. (Currently all IDs must be of the form ^(?:[\\w:-]+\\/)?([\\w:.-]+)-v(\\d+)$.)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Add this as the last thing to do for stable baseline conversion.\n",
    "\n",
    "    model = run_model(N_ROLLOUTS, T)\n",
    "    # Save the model to a desired folder and then delete it to demonstrate loading\n",
    "#     if not os.path.exists(os.path.realpath(os.path.expanduser('~/baseline_results'))):\n",
    "#         os.makedirs(os.path.realpath(os.path.expanduser('~/baseline_results')))\n",
    "#     path = os.path.realpath(os.path.expanduser('~/baseline_results'))\n",
    "#     save_path = os.path.join(path, args.result_name)\n",
    "\n",
    "    self.file = open(filename2, \"w+\")\n",
    "    self.file.close()\n",
    "    print('Saving the trained model!')\n",
    "    model.save(filename2)\n",
    "    del model\n",
    "\n",
    "    # Replay the result by loading the model\n",
    "    print('Loading the trained model and testing it out!')\n",
    "    model = PPO2.load(filename2)\n",
    "#     flow_params = get_flow_params(os.path.join(path, args.result_name) + '.json')\n",
    "#     flow_params['sim'].render = True\n",
    "    env = DummyVecEnv([lambda: BraessEnv()])  # The algorithms require a vectorized environment to run\n",
    "    obs = env.reset()\n",
    "    reward = 0\n",
    "    # Should I have it run only once or multiple times to test it out. I'll probably want to run it multiple times\n",
    "    # Currently HORIZON is only 1\n",
    "    for i in range(HORIZON):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        reward += rewards\n",
    "    print('the final reward is {}'.format(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# MDP: (All characteristics of this MDP is given in W. Krichene's Paper \n",
    "#                            -- \"Learning Nash Equilibria in Congestion Games\")\n",
    "#\n",
    "#  - Observations/States: Each player will observe the cost (travel time) on all of the paths (according to paper)\n",
    "#                         If the player only observes the loss she incurs then it becomes a multiarmed bandit \n",
    "#                         setting.\n",
    "#  - Actions: Each player will choose a path, using a randomized/mixed strategy. \n",
    "#             This means we have a stochastic policy.\n",
    "#  - Reward: The *cost* of each player will be the travel time that they've incurred on their path. T\n",
    "#            Each player wants to minimize their travel time. For reward, we can maximize the negative cost.\n",
    "#  - Model of the environment: We don't have one in this case\n",
    "#\n",
    "###\n",
    "\n",
    "# Test 1: Test that reset() returns an initial observation. \n",
    "#         The initial observation should be:\n",
    "#                \"ABD\": 3\n",
    "#                \"ACD\": 3\n",
    "#                \"ABCD\": 2.25\n",
    "class TestBraessEnv(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        configs = {}\n",
    "        self.env = BraessEnv(configs)\n",
    "#         Representing 1 flow of .01 taking path \"ABD\"\n",
    "#         flow = {\"ABD\": 0.01,\n",
    "#                 \"ACD\": 0,\n",
    "#                 \"ABCD\": 0}\n",
    "        self.action = {\"population_1\": np.array([0.01, 0, 0])}  \n",
    "    \n",
    "#         flows = {\n",
    "#             \"ABD\": 0,\n",
    "#             \"ACD\": 0,\n",
    "#             \"ABCD\": 0.01\n",
    "#         } # Flow of 0.01 taking \"ABCD\"\n",
    "        self.action2 = {\"population_1\": np.array([0, 0, 0.01])}\n",
    "        \n",
    "    def testFormatOfObservationsandRewardsInStepandReset(self):\n",
    "        initial = self.env.reset()\n",
    "        items = list(initial.items())\n",
    "        self.assertEqual(len(items), 1)\n",
    "        \n",
    "        agent, obs = items[0]\n",
    "        \n",
    "        # Seems like the \"preprocessor\" in the reset function turns the dictionary into an numpy array\n",
    "        # Just check if it has 3 values associated with 3 paths\n",
    "        self.assertEqual(len(obs), 3)\n",
    "        \n",
    "        next_obs, reward, done, info = self.env.step(self.action)\n",
    "        \n",
    "        obs_items = list(next_obs.items())\n",
    "        reward_items = list(reward.items())\n",
    "        self.assertEqual(len(obs_items), 1)\n",
    "        self.assertEqual(len(reward_items), 1)\n",
    "        \n",
    "        actual_obs = obs_items[0][1]\n",
    "        actual_reward = reward_items[0][1]\n",
    "        \n",
    "        self.assertEqual(len(actual_obs), 3)\n",
    "        self.assertTrue(isinstance(actual_reward, float))\n",
    "        self.assertTrue(done[\"__all__\"])\n",
    "        \n",
    "        \n",
    "    def testResetReturnsObservation(self):\n",
    "        initial = self.env.reset()       \n",
    "        obs = list(initial.values())[0]\n",
    "        \n",
    "#         dict_expected = {\n",
    "#             \"ABD\": 3,\n",
    "#             \"ACD\": 3,\n",
    "#             \"ABCD\": 2.25\n",
    "#         }\n",
    "        actual_expected = np.array([3, 3, 2.25])\n",
    "        np.testing.assert_array_equal(obs, actual_expected)\n",
    "        \n",
    "    def testStepReturnsCorrectInformation(self):\n",
    "        # Test that step returns correct next observations, reward, and termination signal\n",
    "        env = self.env\n",
    "        env.reset()\n",
    "\n",
    "        next_obs, reward, done, _ = env.step(self.action)\n",
    "        # dict_obs = {\n",
    "        #     \"ABD\": 3.01,\n",
    "        #     \"ACD\": 3,\n",
    "        #     \"ABCD\": 2.26\n",
    "        # }\n",
    "        actual_expected_obs = [3.01, 3, 2.26]\n",
    "        \n",
    "        # travel_times = {\n",
    "        #     \"ABD\": 3.01,\n",
    "        #     \"ACD\": 3,\n",
    "        #     \"ABCD\": 2.26\n",
    "        # }\n",
    "        flow_dist = np.array([0.01, 0, 0])\n",
    "        path_rewards = np.array([-3.01, -3, -2.26]) # Rewards are negative of travel times\n",
    "        expected_reward = np.dot(flow_dist, path_rewards)\n",
    "        \n",
    "        np.testing.assert_array_equal(actual_expected_obs, list(next_obs.values())[0])\n",
    "        self.assertEqual(expected_reward, list(reward.values())[0])\n",
    "        self.assertTrue(done[\"__all__\"])\n",
    "        \n",
    "    def testStepSavesNoPrevInfo(self):\n",
    "        env = self.env\n",
    "        env.reset()\n",
    "        env.step(self.action)\n",
    "    \n",
    "        next_obs, reward, done, _ = env.step(self.action2)\n",
    "        \n",
    "#         dict_obs = {\n",
    "#             \"ABD\": 3.01,\n",
    "#             \"ACD\": 3.01,\n",
    "#             \"ABCD\": 2.27\n",
    "#         }\n",
    "#         travel_times = {\n",
    "#             \"ABD\": 3.01,\n",
    "#             \"ACD\": 3.01,\n",
    "#             \"ABCD\": 2.27\n",
    "#         }\n",
    "        \n",
    "        expected_obs = np.array([3.01, 3.01, 2.27])\n",
    "        \n",
    "        flow_dist = np.array([0, 0, 0.01])\n",
    "        path_rewards = np.array([-3.01, -3.01, -2.27])\n",
    "        expected_reward = np.dot(flow_dist, path_rewards)\n",
    "        \n",
    "        np.testing.assert_array_equal(expected_obs, list(next_obs.values())[0])\n",
    "        self.assertEqual(expected_reward, list(reward.values())[0])\n",
    "        self.assertTrue(done[\"__all__\"])\n",
    "        \n",
    "    def testEnvDoesNotKeepActionsFromPreviousRun(self):\n",
    "#         env = self.env\n",
    "#         # Episode 1\n",
    "#         env.reset()\n",
    "#         env.step(self.action)\n",
    "        \n",
    "#         # Episode 2\n",
    "#         env.reset()\n",
    "# #         flows = {\n",
    "# #             \"ABD\": 0,\n",
    "# #             \"ACD\": 0,\n",
    "# #             \"ABCD\": 0.01\n",
    "# #         } # Flow of 0.01 taking \"ABCD\"\n",
    "# # .     {\"population_1\": np.array([0, 0, 0.01])}\n",
    "    \n",
    "#         env.step(self.action2)\n",
    "        \n",
    "#         # Check\n",
    "#         self.fail()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.265s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABCD': 0, 'ABD': 0, 'ACD': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = BraessNetwork()\n",
    "{route: 0 for route in n.routes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is what was given: {'ABCD': 3.75, 'ABD': 3.75, 'ACD': 3.75}\n",
      "This is what I expect: {'ABCD': 3.75, 'ABD': 3.75, 'ACD': 3.75}\n",
      "---\n",
      "This is what was given: {'ABCD': 3.25, 'ABD': 3.5, 'ACD': 3.5}\n",
      "This is what I expect: {'ABCD': 3.25, 'ABD': 3.5, 'ACD': 3.5}\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "network = BraessNetwork()\n",
    "\n",
    "# Test 1 for calculate ttime\n",
    "#\n",
    "#                  B\n",
    "#                / | \\                   \n",
    "#             /    |    \\\n",
    "#          A       |       D\n",
    "#             \\    |    /\n",
    "#                \\ | /\n",
    "#                  C\n",
    "#\n",
    "# Out of 100 cars, we will do: \n",
    "#     ABD = 25; \n",
    "#     ACD = 25; \n",
    "#     ABCD = 50 \n",
    "# \n",
    "# The travel time on each path should result as:\n",
    "#     ABD = 3.75 (units)\n",
    "#     ACD = 3.75 (units)\n",
    "#     ABCD = 3.75 (units)\n",
    "#\n",
    "flows = {\n",
    "    \"ABD\": 0.25,\n",
    "    \"ACD\": 0.25,\n",
    "    \"ABCD\": 0.50\n",
    "}\n",
    "expect1 = {\n",
    "    \"ABD\": 3.75,\n",
    "    \"ACD\": 3.75,\n",
    "    \"ABCD\": 3.75\n",
    "}\n",
    "times = network.calculate_ttime(flows)\n",
    "print(\"This is what was given: \" + str(times))\n",
    "print(\"This is what I expect: \" + str(expect1))\n",
    "print(\"---\")\n",
    "\n",
    "# Test 2 for calculate ttime\n",
    "# Out of 100 cars, we will do: \n",
    "#     ABD = 50; \n",
    "#     ACD = 50; \n",
    "#     ABCD = 0 \n",
    "# \n",
    "# The travel time on each path should result as:\n",
    "#     ABD = 3.5 (units)\n",
    "#     ACD = 3.5 (units)\n",
    "#     ABCD = 3.25 (units)  - Even though no one's using this path\n",
    "flows = {\n",
    "    \"ABD\": 0.50,\n",
    "    \"ACD\": 0.50,\n",
    "    \"ABCD\": 0\n",
    "}\n",
    "expect2 = {\n",
    "    \"ABD\": 3.5,\n",
    "    \"ACD\": 3.5,\n",
    "    \"ABCD\": 3.25\n",
    "}\n",
    "times = network.calculate_ttime(flows)\n",
    "print(\"This is what was given: \" + str(times))\n",
    "print(\"This is what I expect: \" + str(expect2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to run experiments\n",
    "gamma = 0.5\n",
    "single_pop_network = BraessEnv({})\n",
    "config = {\"gamma\": gamma}\n",
    "policy_graphs = {\n",
    "    'population': (PPOPolicyGraph, single_pop_network.observation_space, single_pop_network.action_space, config)\n",
    "}\n",
    "\n",
    "BRAESS_CONFIG = {\n",
    "    # === Environment ===\n",
    "    # Discount factor of the MDP\n",
    "    \"gamma\": gamma, # Ask Yiling\n",
    "    # Number of steps after which the episode is forced to terminate. Defaults\n",
    "    # to `env.spec.max_episode_steps` (if present) for Gym envs.\n",
    "    \"horizon\": 1,\n",
    "    # Calculate rewards but don't reset the environment when the horizon is\n",
    "    # hit. This allows value estimation and RNN state to span across logical\n",
    "    # episodes denoted by horizon. This only has an effect if horizon != inf.\n",
    "    \"soft_horizon\": True,\n",
    "    # Don't set 'done' at the end of the episode. Note that you still need to\n",
    "    # set this if soft_horizon=True, unless your env is actually running\n",
    "    # forever without returning done=True.\n",
    "    \"no_done_at_end\": True,\n",
    "    # The default learning rate\n",
    "    \"lr\": 0.0001, # Ask Yiling\n",
    "\n",
    "    # === Evaluation ===\n",
    "    # Evaluate with every `evaluation_interval` training iterations.\n",
    "    # The evaluation stats will be reported under the \"evaluation\" metric key.\n",
    "    # Note that evaluation is currently not parallelized, and that for Ape-X\n",
    "    # metrics are already only reported for the lowest epsilon workers.\n",
    "    \"evaluation_interval\": None,\n",
    "    # Number of episodes to run per evaluation period.\n",
    "    \"evaluation_num_episodes\": 1,\n",
    "\n",
    "    # === Multiagent ===\n",
    "    \"multiagent\": {\n",
    "        'policy_graphs': policy_graphs,\n",
    "        # Function mapping agent ids to policy ids.\n",
    "        \"policy_mapping_fn\": tune.function(lambda agent_id: 'population')\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-09-12_01-22-55_2512/logs.\n",
      "Waiting for redis server at 127.0.0.1:54865 to respond...\n",
      "Waiting for redis server at 127.0.0.1:26633 to respond...\n",
      "Starting the Plasma object store with 6.871947672999999 GB memory using /tmp.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=4f93041440b64501ac927342e2dadf9302cfc7dbd9ce3e45\n",
      "======================================================================\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 12.1/17.2 GB\n",
      "\n",
      "Created LogSyncer for /Users/mtgibson/ray_results/route-DQN/PPO_multi_routing_0_2019-09-12_01-22-55t47ksgcr -> \n",
      "WARNING: Falling back to serializing objects of type <class 'numpy.dtype'> by using pickle. This may be inefficient.\n",
      "WARNING: Falling back to serializing objects of type <class 'mtrand.RandomState'> by using pickle. This may be inefficient.\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 12.1/17.2 GB\n",
      "Result logdir: /Users/mtgibson/ray_results/route-DQN\n",
      "RUNNING trials:\n",
      " - PPO_multi_routing_0:\tRUNNING\n",
      "\n",
      "Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/marl_routing/lib/python3.5/site-packages/ray/tune/trial_runner.py\", line 261, in _process_events\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/anaconda3/envs/marl_routing/lib/python3.5/site-packages/ray/tune/ray_trial_executor.py\", line 211, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/anaconda3/envs/marl_routing/lib/python3.5/site-packages/ray/worker.py\", line 2386, in get\n",
      "    raise value\n",
      "ray.worker.RayTaskError: \u001b[36mray_PPOAgent:train()\u001b[39m (pid=2533, host=C02X23BZJHD3)\n",
      "  File \"/anaconda3/envs/marl_routing/lib/python3.5/site-packages/ray/rllib/agents/agent.py\", line 244, in __init__\n",
      "    Trainable.__init__(self, config, logger_creator)\n",
      "  File \"/anaconda3/envs/marl_routing/lib/python3.5/site-packages/ray/tune/trainable.py\", line 87, in __init__\n",
      "    self._setup(copy.deepcopy(self.config))\n",
      "  File \"/anaconda3/envs/marl_routing/lib/python3.5/site-packages/ray/rllib/agents/agent.py\", line 304, in _setup\n",
      "    self._allow_unknown_subkeys)\n",
      "  File \"/anaconda3/envs/marl_routing/lib/python3.5/site-packages/ray/rllib/utils/__init__.py\", line 35, in deep_update\n",
      "    raise Exception(\"Unknown config parameter `{}` \".format(k))\n",
      "Exception: Unknown config parameter `soft_horizon` \n",
      "\n",
      "Worker ip unknown, skipping log sync for /Users/mtgibson/ray_results/route-DQN/PPO_multi_routing_0_2019-09-12_01-22-55t47ksgcr\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 13.4/17.2 GB\n",
      "Result logdir: /Users/mtgibson/ray_results/route-DQN\n",
      "ERROR trials:\n",
      " - PPO_multi_routing_0:\tERROR, 1 failures: /Users/mtgibson/ray_results/route-DQN/PPO_multi_routing_0_2019-09-12_01-22-55t47ksgcr/error_2019-09-12_01-23-04.txt\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 13.4/17.2 GB\n",
      "Result logdir: /Users/mtgibson/ray_results/route-DQN\n",
      "ERROR trials:\n",
      " - PPO_multi_routing_0:\tERROR, 1 failures: /Users/mtgibson/ray_results/route-DQN/PPO_multi_routing_0_2019-09-12_01-22-55t47ksgcr/error_2019-09-12_01-23-04.txt\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_multi_routing_0])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-11a95f958c76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     }\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mrun_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/marl_routing/lib/python3.5/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(experiments, search_alg, scheduler, with_server, server_port, verbose, queue_trials, trial_executor, raise_on_failed_trial)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrored_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_multi_routing_0])"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env_creator_name = 'multi_routing'\n",
    "    register_env(env_creator_name, lambda config: BraessEnv(config))\n",
    "    ray.init()\n",
    "    experiments = {\n",
    "        'route-DQN': {\n",
    "            'run': 'PPO',\n",
    "            'env': 'multi_routing',\n",
    "            'stop': {\n",
    "                'training_iteration': 5\n",
    "            },\n",
    "            'config': BRAESS_CONFIG\n",
    "        },\n",
    "        # put additional experiments to run concurrently here\n",
    "    }\n",
    "    \n",
    "    run_experiments(experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots \n",
    "\n",
    "The following code plots the RL results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "file = open(filename, 'r')\n",
    "j = 0\n",
    "# we want to plot the evolution of the flow distribution, of the travel time, and of the reward/average travel time\n",
    "Actions_plot = np.array([[0, 0, 0]]) # flow path\n",
    "Reward_plot = np.array([0]) # average travel time/cost or reward\n",
    "Travel_time_plot = np.array([[0, 0, 0]]) # travel time on each path\n",
    "while(True):\n",
    "    j = j+1\n",
    "    data = file.readline()\n",
    "#     if j%100 != 1:\n",
    "#         continue\n",
    "    try:\n",
    "#         action_dict = ast.literal_eval(\"{\" + actions.split('{')[1].split('}')[0]+ \"}\")\n",
    "#         reward_dict = ast.literal_eval(\"{\" + rewards.split('{')[1].split('}')[0]+ \"}\")\n",
    "        \n",
    "        data = data.split(';')\n",
    "        \n",
    "    \n",
    "#         network = Networks.network(network_name, nb_veh)\n",
    "#         travel_time, marginal_cost, rew_dict = get_tt_mc(action_dict, network, 0)\n",
    "        \n",
    "#         actions_np = np.fromiter(action_dict.values(), dtype=int)\n",
    "#         Actions_plot = np.append(Actions_plot, [actions_np], axis=0)\n",
    "#         rewards_np = np.fromiter(reward_dict.values(), dtype=float)\n",
    "#         Reward_plot = np.append(Reward_plot, [rewards_np], axis=0)\n",
    "#         travel_time_np = np.fromiter(travel_time.values(), dtype=float)\n",
    "#         Travel_time_plot = np.append(Travel_time_plot, [travel_time_np], axis=0)\n",
    "#         if(j==1):\n",
    "#             print(\"------ First iteration ------\")\n",
    "#             print(\"Path choice: \" + str(action_dict))\n",
    "#             print(\"Reward ray: \" + str(reward_dict))\n",
    "#             print(\"Travel time paths: \" + str({\"path \" + str(i): network.travel_time(i) for i in range(3)}))\n",
    "#             print(\"Travel time cars: \" + str(travel_time))\n",
    "#             print(\"Marginal cost: \" + str(marginal_cost))\n",
    "#             print(\"Reward network: \" + str(rew_dict))\n",
    "    except:\n",
    "        print()\n",
    "        print(\"------ Last iteration ------\")\n",
    "        print(\"Path choice: \" + str(action_dict))\n",
    "        print(\"Reward ray: \" + str(reward_dict))\n",
    "        print(\"Travel time paths: \" + str({\"path \" + str(i): network.travel_time(i) for i in range(3)}))\n",
    "        print(\"Travel time cars: \" + str(travel_time))\n",
    "        print(\"Marginal cost: \" + str(marginal_cost))\n",
    "        print(\"Reward network: \" + str(rew_dict))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
